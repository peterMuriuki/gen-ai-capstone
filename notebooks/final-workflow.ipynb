{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73798c-ff71-4b80-9e17-4338497522b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "\n",
    "First, install ChromaDB and the Gemini API Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16edec1-7f10-4386-a87b-1ff60397b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n",
    "!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\" \"requests==2.32.3\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8ce21-1b73-4966-941b-d570f32a94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4ea09-dc8e-458d-8a87-70bdbfc873c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d848d2-0ffc-478d-a4ee-fd02e8636e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f71ec-1170-42a2-85ec-5e03ea497f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environs[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f52747-cebb-40ee-8e1d-a33c52a638f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "HAPI_FHIR_BASE_URL = \"https://hapi.fhir.org/baseR4\"\n",
    "QUESTIONNAIRE_ENDPOINT = f\"{HAPI_FHIR_BASE_URL}/Questionnaire\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371956e8-e474-45c4-9e22-c2fccba5cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data\n",
    "\n",
    "Discover the questionnaire metadata that we will use to create an embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa050ee6-6a3c-411c-b352-da10c5a98e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_questionnaires_from_hapi() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches all Questionnaire resources from the HAPI FHIR server.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of Questionnaire resources in JSON format.\n",
    "                     Returns an empty list if there's an error.\n",
    "    \"\"\"\n",
    "    questionnaires = []\n",
    "    try:\n",
    " # count_params = {\"_summary\": \"count\"}\n",
    "        # response = requests.get(QUESTIONNAIRE_ENDPOINT, params=count_params)\n",
    "        # questionnaire_count = response.json()[\"total\"]\n",
    "        questionnaire_count = 100\n",
    "        all_params = {\"_count\": questionnaire_count, \"_sort\": \"-_lastUpdated\", \"_element\":\"description,id,identifier,name,title\"}\n",
    "        response = requests.get(QUESTIONNAIRE_ENDPOINT,params=all_params, headers={\"Accept\": \"application/fhir+json\"})\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        bundle = response.json()\n",
    "\n",
    "        if bundle.get('resourceType') == 'Bundle' and bundle.get('type') == 'searchset':\n",
    "            for entry in bundle.get('entry', []):\n",
    "                if entry.get('resource') and entry['resource'].get('resourceType') == 'Questionnaire':\n",
    "                    questionnaires.append(entry['resource'])\n",
    "        else:\n",
    "            print(f\"Unexpected response format from FHIR server: {bundle.get('resourceType')}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching Questionnaires from HAPI FHIR: {e}\")\n",
    "        return []  # Return empty list in case of error\n",
    "\n",
    "    return questionnaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164a7d6-417d-4e1a-ae25-649d5f49bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the embedding database with ChromaDB\n",
    "\n",
    "We create a [custom function](https://docs.trychroma.com/guides/embeddings#custom-embedding-functions) to generate embeddings with the Gemini API. \n",
    "\n",
    "The questionnaire metadata are the items that are in the database. They are inserted first, and later retrieved. Queries will be a description of the form to be filled derived from the prompt instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9279a-aa5f-4821-96d0-ac08fea942d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    document_mode = True\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            contents=input,\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=embedding_task,\n",
    "            ),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9755e-5d7a-4a15-bad1-d8926c707457",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now create a [Chroma database client](https://docs.trychroma.com/getting-started) that uses the `GeminiEmbeddingFunction` and populate the database with the questionnaire metadata from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b8cfc-ee27-4017-a56c-65e64f798f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "DB_NAME = \"fhir-questionnaire\"\n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "questionnare_documents = fetch_questionnaires_from_hapi()\n",
    "\n",
    "db.add(documents=questionnare_documents, ids=[quest.get(\"id\") for quest in questionnare_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95540e56-f619-4ca6-b690-a1c519469846",
   "metadata": {},
   "outputs": [],
   "source": [
    "Confirm that the data was inserted by looking at the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000694e-9d36-4cd0-8c73-235a34035a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.count()\n",
    "# You can peek at the data too.\n",
    "# db.peek(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02dbc5-5291-440a-801e-8150ae51b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieval: Finding relevant questionnaires\n",
    "\n",
    "We can then use the prompt to get the questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89f9dc-2396-4e24-96ad-f07c080703ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_relevant_quest(query):\n",
    "    embed_fn.document_mode = false\n",
    "    result = db.query(query_texts=[query], n_results=1)\n",
    "    # TODO -> how we parse the results here\n",
    "    [all_passages] = result[\"documents\"]\n",
    "    return all_passages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c56fa-5a23-4d2f-89f5-7371d3d6281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the questionnaire and the transcripted audio files, we can move on to generate the questionnaireResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7030480-db47-4d1c-8c63-fd7c3ee69096",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m qr_client = \u001b[43mgenai\u001b[49m.Client(api_key=GOOGLE_API_KEY)\n\u001b[32m      2\u001b[39m qr_client_model_id = \u001b[33m\"\u001b[39m\u001b[33mgemini-2.0-flash\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'genai' is not defined"
     ]
    }
   ],
   "source": [
    "qr_client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "qr_client_model_id = \"gemini-2.0-flash\"\n",
    "\n",
    "def direct_text_to_qr(transcribed: str, questionnaire_template: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extract relevant information,\n",
    "    and return a FHIR QuestionnaireResponse resource as a dict.\n",
    "    \"\"\"\n",
    "    # 3. Prepare the LLM prompt\n",
    "    prompt = create_prompt_for_questionnaire_response(\n",
    "        transcribed, questionnaire_template\n",
    "    )\n",
    "\n",
    "    response = qr_client.models.generate_content(\n",
    "        model=qr_client_model_id, contents=[prompt, transcribed], config={\n",
    "            'response_mime_type': 'application/json'\n",
    "        }\n",
    "    )\n",
    "    response = transcription_client.generate_content(\n",
    "            model=transcription_model_id,\n",
    "            contents = [prompt, uploaded_audio_file]\n",
    "        )\n",
    "\n",
    "    qr_string = response.text.strip()\n",
    "    qr = repair_json.loads(qr_string)\n",
    "    # try:\n",
    "    #     questionnaire_response = json.loads(llm_output)\n",
    "    # except Exception as e:\n",
    "    #     raise ValueError(f\"Invalid JSON from LLM: {e}\")\n",
    "\n",
    "    # # 6. Validate the JSON against FHIR schema (optional but recommended)\n",
    "    # #    This step ensures the object meets the QuestionnaireResponse structure\n",
    "    # if not validate_fhir_questionnaire_response(questionnaire_response):\n",
    "    #     raise ValueError(\"Generated QuestionnaireResponse is not valid FHIR.\")\n",
    "\n",
    "    # # 7. Return or store the final resource\n",
    "    return qr\n",
    "\n",
    "\n",
    "def create_prompt_for_questionnaire_response(\n",
    "    cleaned_text: str, questionnaire_template: dict\n",
    ") -> str:\n",
    "    # Construct a system/user prompt with instructions,\n",
    "    # referencing relevant sections of the conversation\n",
    "    prompt = f\"\"\"\n",
    "    You are a medical documentation assistant.\n",
    "    Below is a transcribed patient-physician conversation:\n",
    "    ---\n",
    "    {cleaned_text}\n",
    "    ---\n",
    "\n",
    "    You have a FHIR Questionnaire defined as follows:\n",
    "    {json.dumps(questionnaire_template, indent=2)}\n",
    "\n",
    "    Extract the relevant data from the conversation to populate a FHIR QuestionnaireResponse\n",
    "    based on the provided Questionnaire. Return ONLY valid JSON representing this \n",
    "    QuestionnaireResponse with fields \"resourceType\": \"QuestionnaireResponse\", \n",
    "    \"questionnaire\": \"<Questionnaire-identifier>\",\n",
    "    \"status\", \"subject\", \"authored\", \"item\", etc.\n",
    "\n",
    "    If a field is unknown, leave it blank or null. \n",
    "    Do not add additional commentary.\n",
    "\n",
    "    Use this JSON schema:\n",
    "\n",
    "    QuestionnaireResponse = <generated questionnaireResponse>\n",
    "    return: QuestionnaireResponse\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af628e5f-1b4e-4f11-a4a6-006f2a62ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_qr(qr):\n",
    "    # use a publicly available fhir instance.\n",
    "    url = \"http://hapi.fhir.org/baseR4/Patient/$validate\"\n",
    "    headers = {\"Content-Type\": \"application/fhir+json\"}\n",
    "\n",
    "    response = requests.post(url, json=qr, headers=headers)\n",
    "    return response.ok():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21fbad-fae6-4732-acee-9c0d8481215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_questionnaire_response(questionnaire_response: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Saves the validated QuestionnaireResponse to the HAPI FHIR server.\n",
    "\n",
    "    Args:\n",
    "        questionnaire_response (Dict): The validated FHIR QuestionnaireResponse in JSON format.\n",
    "\n",
    "    Returns:\n",
    "        str: A success message or an error message if saving fails.\n",
    "    \"\"\"\n",
    "    print(\"Saving QuestionnaireResponse to HAPI FHIR...\")\n",
    "    questionnaire_response_endpoint = f\"{HAPI_FHIR_BASE_URL}/QuestionnaireResponse\"\n",
    "    headers = {\"Accept\": \"application/fhir+json\", \"Content-Type\": \"application/fhir+json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(questionnaire_response_endpoint, headers=headers, json=questionnaire_response)\n",
    "        response.raise_for_status() # Raise exception for HTTP errors\n",
    "\n",
    "        created_resource = response.json()\n",
    "        if created_resource.get('resourceType') == 'QuestionnaireResponse' and response.status_code == 201: # Check for 201 Created status\n",
    "            resource_id = created_resource.get('id')\n",
    "            return f\"QuestionnaireResponse saved successfully with ID: {resource_id}\"\n",
    "        else:\n",
    "            return f\"Error saving QuestionnaireResponse. FHIR server response: {created_resource.get('resourceType')}, Status Code: {response.status_code}\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error saving QuestionnaireResponse to HAPI FHIR: {e}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
