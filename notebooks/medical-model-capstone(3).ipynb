{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Clinical Documentation Assistant\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "Medical documentation consumes a significant amount of healthcare professionals' time, contributing to burnout and reducing time available for direct patient care. This notebook demonstrates an AI-powered workflow designed to alleviate this burden.\n",
    "\n",
    "**Goal:** Automatically process audio recordings of physician-patient encounters to:\n",
    "1.  **Extract structured medical information.**\n",
    "2.  **Pre-fill relevant clinical forms** (e.g., medical history questionnaires).\n",
    "3.  **Generate standardized clinical notes** (e.g., SOAP notes).\n",
    "4.  **Output data in FHIR format** for seamless integration with Electronic Health Records (EHRs) and other healthcare systems.\n",
    "\n",
    "By automating these tasks, we aim to improve efficiency, reduce administrative overhead, and enhance data quality and reusability for clinical workflows and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Architecture\n",
    "\n",
    "This solution employs a multi-step process orchestrated using LangGraph, a framework for building stateful, multi-actor applications with LLMs.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1.  **Questionnaire Discovery (RAG):**\n",
    "    *   Uses Retrieval-Augmented Generation (RAG) to find the most relevant FHIR [Questionnaire](https://www.hl7.org/fhir/R4/questionnaire.html) based on user instructions (e.g., \"Fill out a medical history report\").\n",
    "    *   Leverages ChromaDB (a vector database) and Gemini embeddings to store and search questionnaire descriptions.\n",
    "    *   Includes a validation step using Gemini to confirm the relevance of the retrieved questionnaire.\n",
    "2.  **Audio Processing & Form Filling:**\n",
    "    *   Uploads the encounter audio file to the Gemini API.\n",
    "    *   Uses a multimodal Gemini model to analyze the audio content and the selected FHIR Questionnaire schema.\n",
    "    *   Generates a FHIR [QuestionnaireResponse](https://www.hl7.org/fhir/R4/questionnaireresponse.html), representing the completed form based on information extracted from the audio.\n",
    "3.  **SOAP Note Generation:**\n",
    "    *   Uses the Gemini API to generate a SOAP (Subjective, Objective, Assessment, Plan) note directly from the audio recording.\n",
    "4.  **FHIR Resource Creation:**\n",
    "    *   Creates FHIR [Binary](https://www.hl7.org/fhir/R4/binary.html) and [DocumentReference](https://www.hl7.org/fhir/R4/documentreference.html) resources to represent the generated SOAP note in a standard, interoperable format suitable for EHR integration.\n",
    "\n",
    "**Note:** For demonstration purposes, a local JSON file (`quest.db.json`) acts as a placeholder repository for FHIR Questionnaires instead of a live FHIR server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### 1.1 Install Required Packages\n",
    "\n",
    "Install the necessary Python libraries for interacting with the Gemini API, ChromaDB, LangChain, LangGraph, and handling JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:15:30.864921Z",
     "iopub.status.busy": "2025-04-19T21:15:30.864636Z",
     "iopub.status.idle": "2025-04-19T21:16:33.328139Z",
     "shell.execute_reply": "2025-04-19T21:16:33.326715Z",
     "shell.execute_reply.started": "2025-04-19T21:15:30.864891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n",
    "%pip install -q \"google-genai==1.7.0\" \"chromadb==0.6.3\" \"langchain==0.3.23\" \"langgraph==0.3.29\" \"json-repair==0.41.1\" \"google-api-core==2.24.2\" \"langchain-google-genai==2.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Setting Up `GOOGLE_API_KEY` for Execution\n",
    "\n",
    "To successfully run the next cell, you must provide your `GOOGLE_API_KEY`. This is required to authenticate with Google APIs and is handled differently depending on the environment:\n",
    "\n",
    "- **Google Colab**\n",
    "  1.TBD\n",
    "\n",
    "- **Kaggle**\n",
    "  1. Click on **\"Add-ons\" > \"Secrets\"** in the notebook editor.\n",
    "  2. Create a new secret with the name `GOOGLE_API_KEY`.\n",
    "  3. Paste your API key as the value.\n",
    "  4. The notebook will automatically retrieve the secret.\n",
    "\n",
    "- **Local Environment**\n",
    "  1. Set the `GOOGLE_API_KEY` as an environment variable:\n",
    "     - Temporarily (for a session):\n",
    "       ```bash\n",
    "       export GOOGLE_API_KEY=<your-api-key>\n",
    "       ```\n",
    "  2. Restart your terminal or IDE session if needed before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:33.332041Z",
     "iopub.status.busy": "2025-04-19T21:16:33.331671Z",
     "iopub.status.idle": "2025-04-19T21:16:33.429838Z",
     "shell.execute_reply": "2025-04-19T21:16:33.428974Z",
     "shell.execute_reply.started": "2025-04-19T21:16:33.332009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Case 1: Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
    "except ImportError:\n",
    "    # Not in Colab\n",
    "    pass\n",
    "\n",
    "# Case 2: Kaggle (use kaggle secrets)\n",
    "if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secret = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = secret\n",
    "    except Exception as e:\n",
    "        print(\"Kaggle secret 'GOOGLE_API_KEY' not found or could not be retrieved:\", e)\n",
    "\n",
    "# Case 3: Local dev - assume manually set in environment\n",
    "GOOGLE_API_KEY=os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"GOOGLE_API_KEY not found. Please set it in your environment variables \"\n",
    "        \"or in Kaggle/Colab secrets.\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:33.431346Z",
     "iopub.status.busy": "2025-04-19T21:16:33.430984Z",
     "iopub.status.idle": "2025-04-19T21:16:35.581859Z",
     "shell.execute_reply": "2025-04-19T21:16:35.580773Z",
     "shell.execute_reply.started": "2025-04-19T21:16:33.431303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "model_id = \"gemini-2.0-flash\"\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "# setup a retry scheme in case some requests fail (mostly due to rate limiting)\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Questionnaire Data & Vector Store\n",
    "\n",
    "This section focuses on setting up the data source for FHIR Questionnaires and creating a vector database (ChromaDB) to enable semantic search for relevant forms.\n",
    "\n",
    "### 2.1 Define Gemini Embedding Function for ChromaDB\n",
    "\n",
    "ChromaDB needs a way to convert text (questionnaire descriptions) into numerical vectors (embeddings) for similarity searching. We define a custom embedding function using the Gemini `text-embedding-004` model.\n",
    "\n",
    "*   The function handles embedding generation for both `retrieval_document` (when adding questionnaires to the DB) and `retrieval_query` (when searching the DB).\n",
    "*   It includes retry logic (`@retry.Retry`) to handle potential transient API errors (like rate limits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:35.583592Z",
     "iopub.status.busy": "2025-04-19T21:16:35.582979Z",
     "iopub.status.idle": "2025-04-19T21:16:36.270698Z",
     "shell.execute_reply": "2025-04-19T21:16:36.269786Z",
     "shell.execute_reply.started": "2025-04-19T21:16:35.583554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    document_mode = True\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            contents=input,\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=embedding_task,\n",
    "            ),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Questionnaire Data\n",
    "\n",
    "Define utility functions to read the FHIR Questionnaire data from the local JSON file (`../quest.db.json`).\n",
    "\n",
    "*   `read_questionnaires_from_fs()`: Reads the JSON file (cached for efficiency).\n",
    "*   `get_quest_docs_meta()`: Extracts the `description` (or uses \"No description\" if missing) and relevant `metadata` (id, title, name) for each questionnaire. This prepares the data for insertion into ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:36.272040Z",
     "iopub.status.busy": "2025-04-19T21:16:36.271670Z",
     "iopub.status.idle": "2025-04-19T21:16:36.279986Z",
     "shell.execute_reply": "2025-04-19T21:16:36.279020Z",
     "shell.execute_reply.started": "2025-04-19T21:16:36.272013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\"\"\" \n",
    "Utility functions to read FHIR Questionnaire data from a local JSON file.\n",
    "\"\"\"\n",
    "_quest_docs = None\n",
    "def read_questionnaires_from_fs():\n",
    "    global _quest_docs\n",
    "    if _quest_docs is None:\n",
    "        with open(\"/kaggle/input/quest-sample-db/quest.db.json\", \"r\") as file:\n",
    "            _quest_docs = json.loads(file.read())\n",
    "    return _quest_docs\n",
    "\n",
    "def get_quest_docs_meta():\n",
    "    quest_docs = read_questionnaires_from_fs()\n",
    "    doc_with_metad = []\n",
    "    doc_ids = []\n",
    "    for doc in quest_docs:\n",
    "        doc_id = doc.get(\"id\")\n",
    "        doc_meta = {\n",
    "            k: v\n",
    "            for k, v in {\n",
    "                \"id\": doc_id,\n",
    "                \"title\": doc.get(\"title\"),\n",
    "                \"name\": doc.get(\"name\"),\n",
    "            }.items()\n",
    "            if v is not None\n",
    "        }\n",
    "        doc_desc = (\n",
    "            doc.get(\"description\") if doc.get(\"description\") else \"No description\"\n",
    "        )\n",
    "        doc_with_metad.append((doc_desc, doc_meta))\n",
    "        doc_ids.append(doc_id)\n",
    "    return doc_with_metad, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Populate ChromaDB Vector Store\n",
    "\n",
    "Now, we initialize the ChromaDB client and create/get a collection named `fhir-quest-semantic` using our custom Gemini embedding function.\n",
    "\n",
    "The `populate_vector_db` function:\n",
    "1.  Retrieves the questionnaire descriptions and metadata using `get_quest_docs_meta()`.\n",
    "2.  Sets the embedding function mode to `retrieval_document`.\n",
    "3.  Adds the descriptions as documents to the ChromaDB collection, using the questionnaire `id` as the document ID and storing the extracted `title` and `name` as metadata associated with each vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:36.281325Z",
     "iopub.status.busy": "2025-04-19T21:16:36.280995Z",
     "iopub.status.idle": "2025-04-19T21:16:37.182460Z",
     "shell.execute_reply": "2025-04-19T21:16:37.181619Z",
     "shell.execute_reply.started": "2025-04-19T21:16:36.281294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "DB_NAME = \"fhir-quest-semantic\"\n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "\n",
    "def populate_vector_db():\n",
    "    \"\"\"\n",
    "    Populates the ChromaDB vector store with FHIR questionnaire data.\n",
    "    \"\"\"\n",
    "    embed_fn.document_mode = True\n",
    "    (desc_with_metad, doc_ids) = get_quest_docs_meta()\n",
    "    descriptions, meta = zip(*desc_with_metad)\n",
    "\n",
    "    db.add(documents=list(descriptions), ids=doc_ids, metadatas=list(meta))\n",
    "\n",
    "\n",
    "populate_vector_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Verify Database Population\n",
    "\n",
    "Confirm that the questionnaires have been successfully added to the ChromaDB collection by checking the document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.185727Z",
     "iopub.status.busy": "2025-04-19T21:16:37.185401Z",
     "iopub.status.idle": "2025-04-19T21:16:37.196941Z",
     "shell.execute_reply": "2025-04-19T21:16:37.195723Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.185701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "db.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Retrieval: Finding relevant questionnaires\n",
    "\n",
    "We will be using the user prompt to find a relevant questionnaire to fill. We do so by\n",
    "\n",
    "1. Querying our vector store for the questionnaire that is most semantically related to the users needs\n",
    "2. Then use the gemini model to validate that the questionnaire does actually relate to the users prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.198345Z",
     "iopub.status.busy": "2025-04-19T21:16:37.197948Z",
     "iopub.status.idle": "2025-04-19T21:16:37.213110Z",
     "shell.execute_reply": "2025-04-19T21:16:37.212284Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.198293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_form_validation_prompt(user_prompt, quest_desc, quest_metadata):\n",
    "    \"\"\"\n",
    "    Generates a prompt for validating the relevance of a questionnaire.\n",
    "    This function constructs a prompt for the Gemini model to evaluate the relevance of a FHIR Questionnaire\n",
    "    The model is expected to determine if the questionnaire is likely to be relevant and useful for the user's instruction.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "# Instruction\n",
    "You are an evaluator. Your task is to evaluate the relevance of a form description and metadata to a user instruction.\n",
    "We will provide you with the user instruction, and the form description and metadata.\n",
    "Read the user instruction carefully to understand the user's need, and then evaluate if the provided form description and metadata are relevant to fulfilling that need based on the Criteria provided in the Evaluation section below.\n",
    "You will assign the form description a rating following the Rating Rubric\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "You will be assessing form relevance, which measures whether the provided form description and metadata are suitable for fulfilling the user's instruction. Relevance implies that a user could likely find the form useful and pertinent to their stated need.\n",
    "\n",
    "## Criteria\n",
    "Relevance to User Instruction: The form description and metadata align with the user's instruction and suggest the form could potentially address the user's need.\n",
    "Usefulness for User Instruction: The form, as described, appears practically useful for a user attempting to follow the given instruction.\n",
    "Clarity of Description: The form description and metadata are clear and understandable enough to assess relevance. (If description is unclear, down-rate even if potentially relevant).\n",
    "\n",
    "## Rating Rubric\n",
    "(YES). The form is very likely to be relevant and useful for the user instruction. The description is clear and strongly suggests a good match.\n",
    "(NO). The form is not relevant to the user instruction. The description clearly indicates the form is unrelated to the user's need.\n",
    "\n",
    "# User Inputs and Model Rating\n",
    "## User Instruction\n",
    "\n",
    "### Prompt\n",
    "{user_prompt}\n",
    "\n",
    "## Form Description and Metadata\n",
    "\n",
    "### Form Instruction Description\n",
    "{quest_desc}\n",
    "\n",
    "### Form Metadata (JSON)\n",
    "{quest_metadata}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.214665Z",
     "iopub.status.busy": "2025-04-19T21:16:37.214314Z",
     "iopub.status.idle": "2025-04-19T21:16:37.240920Z",
     "shell.execute_reply": "2025-04-19T21:16:37.239998Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.214642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "\"\"\"\n",
    "Enumeration to represent the relevance rating of a questionnaire.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RelevantRating(enum.Enum):\n",
    "    YES = \"Yes\"\n",
    "    NO = \"No\"\n",
    "\n",
    "\n",
    "def discover_questionnaire(query):\n",
    "    \"\"\"\n",
    "    Discovers a relevant questionnaire by querying the ChromaDB vector store and validating the result with the Gemini model.\n",
    "    \"\"\"\n",
    "    embed_fn.document_mode = False\n",
    "    result = db.query(query_texts=[query], n_results=1)\n",
    "    queried_doc_ids = result.get(\"ids\")\n",
    "    try:\n",
    "        interest_doc_id = queried_doc_ids[0][0]\n",
    "    except IndexError:\n",
    "        return None\n",
    "    queried_doc_desc = result.get(\"documents\")[0][0]\n",
    "    queried_doc_meta = result.get(\"metadatas\")[0][0]\n",
    "    prompt = generate_form_validation_prompt(query, queried_doc_desc, queried_doc_meta)\n",
    "\n",
    "    structured_output_config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=RelevantRating,\n",
    "    )\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id, contents=[prompt], config=structured_output_config\n",
    "    )\n",
    "    parsed_resp = response.parsed\n",
    "\n",
    "    if parsed_resp is RelevantRating.YES:\n",
    "        return interest_doc_id\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Define the langraph workflow\n",
    "\n",
    "We define the typing for our graph state in preparationto defining the function nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.242767Z",
     "iopub.status.busy": "2025-04-19T21:16:37.242293Z",
     "iopub.status.idle": "2025-04-19T21:16:37.773036Z",
     "shell.execute_reply": "2025-04-19T21:16:37.772122Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.242742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\"\"\"\n",
    "Defines the structure of the agent's internal state for tracking during execution.\n",
    "\"\"\"\n",
    "from typing_extensions import TypedDict, Any, Dict, List\n",
    "\n",
    "# Define the state of our graph\n",
    "class AgentState(TypedDict):\n",
    " audio_file_path: str\n",
    " uploaded_audio_file: Any\n",
    " instructions: str\n",
    " quest: Dict[str, Any]\n",
    " patient_res: Dict[str, Any]\n",
    " practitioner_res: Dict[str, Any]\n",
    " quest_resp: str\n",
    " quest_found: bool\n",
    " soap_note: str\n",
    " soap_fhir_resources: List[Dict[str, Any]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Define nodes to use in our graph workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.774390Z",
     "iopub.status.busy": "2025-04-19T21:16:37.774119Z",
     "iopub.status.idle": "2025-04-19T21:16:37.780966Z",
     "shell.execute_reply": "2025-04-19T21:16:37.779609Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.774369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fetch_questionnaire(state: AgentState):\n",
    "    \"\"\"\n",
    "    Fetches a relevant questionnaire based on a given query, using the `discover_questionnaire` function and reading from the local file system.\n",
    "    \"\"\"\n",
    "    query = state.get(\"instructions\")\n",
    "    quest_id = discover_questionnaire(query)\n",
    "\n",
    "    full_quest_docs = read_questionnaires_from_fs()\n",
    "    of_interest_quest = None\n",
    "    for quest in full_quest_docs:\n",
    "        if quest[\"id\"] == quest_id:\n",
    "            of_interest_quest = quest\n",
    "            break\n",
    "    if of_interest_quest is None:\n",
    "        return {\"quest_found\": False}\n",
    "    else:\n",
    "        return {\"quest_found\": True, \"quest\": of_interest_quest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.782327Z",
     "iopub.status.busy": "2025-04-19T21:16:37.782071Z",
     "iopub.status.idle": "2025-04-19T21:16:37.808499Z",
     "shell.execute_reply": "2025-04-19T21:16:37.807660Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.782305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "_upload_file_cache = None\n",
    "\n",
    "def upload_to_gemini(state: AgentState):\n",
    "    \"\"\"\n",
    "    Uploads the local audio file to Gemini if not already uploaded.\n",
    "    Returns a dictionary with the uploaded file object.\n",
    "    \"\"\"\n",
    "    global _upload_file_cache\n",
    "    local_file_path = state.get(\"audio_file_path\")\n",
    "\n",
    "    try:\n",
    "        if _upload_file_cache is None:\n",
    "            _upload_file_cache = client.files.upload(file=local_file_path)\n",
    "        return {\"uploaded_audio_file\": _upload_file_cache}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to Gemini: {e}\")\n",
    "        # You can also return None, raise the error, or log it more formally\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.809992Z",
     "iopub.status.busy": "2025-04-19T21:16:37.809512Z",
     "iopub.status.idle": "2025-04-19T21:16:37.835499Z",
     "shell.execute_reply": "2025-04-19T21:16:37.834413Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.809963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json_repair\n",
    "\n",
    "def get_questionnaire_response(state: AgentState):\n",
    "    \"\"\"\n",
    "    Use gen ai to generate a questionnaireResponse(form submission instance) from the audio file and using a\n",
    "    questionnaire discovered in a previous step.\n",
    "    \"\"\"\n",
    "    audio_file = state.get(\"uploaded_audio_file\")\n",
    "    questionnaire = state.get(\"quest\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        You are an audio processing expert with extensive experience in converting audio files into structured data formats, specifically JSON. Your specialty lies in accurately extracting meaningful information from audio recordings and populating questionnaire-style data structures based on that information.\n",
    "        \n",
    "        Your task is to analyze the provided audio file and patient Electronic Medical Record (EMR) and fill out the questionnaire with the relevant responses. The output format should follow the structure of the provided questionnaireResponse example.\n",
    "\n",
    "        \n",
    "        Here is the questionnaire:\n",
    "        {questionnaire}\n",
    "\n",
    "        \n",
    "        Please analyze the audio and generate the appropriate questionnaire response.\n",
    "\n",
    "        Use this JSON schema:\n",
    "\n",
    "        QuestionnaireResponse = <generated questionnaireResponse>\n",
    "        return: QuestionnaireResponse\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[audio_file, prompt],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            response_mime_type='application/json',\n",
    "        )\n",
    "    )\n",
    "    qr = json_repair.loads(response.text)\n",
    "    return {\"quest_resp\": qr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.836810Z",
     "iopub.status.busy": "2025-04-19T21:16:37.836579Z",
     "iopub.status.idle": "2025-04-19T21:16:37.857597Z",
     "shell.execute_reply": "2025-04-19T21:16:37.856337Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.836791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "soap_note_generation_sys_prompt = \"\"\"You are an expert medical scribe tasked with generating a concise and accurate SOAP (Subjective, Objective, Assessment, Plan) note from a health care provider - patient conversation.\n",
    "**Input:** You will be provided with an audio recording of the conversation.\n",
    "**Task:**  Analyze the transcription and extract relevant information to populate each section of a SOAP note.\n",
    "**Output:**  Generate a SOAP note in the following structured format:\n",
    "S - Subjective:\n",
    "    Chief Complaint (CC): [Concise statement of the patient's primary reason for visit]\n",
    "    History of Present Illness (HPI): [Detailed narrative of the patient's current problem, using OLDCARTS or similar mnemonic if applicable. Include onset, location, duration, character, aggravating/alleviating factors, radiation, timing, severity.]\n",
    "    Past Medical History (PMH): [Summarize relevant past medical conditions mentioned by the patient or provider.]\n",
    "    Medications: [List current medications mentioned by the patient.]\n",
    "    Allergies: [List known allergies mentioned by the patient.]\n",
    "    Social History (SH): [Extract pertinent social history details like smoking, alcohol use, occupation, living situation if discussed and relevant to the encounter.]\n",
    "    Family History (FH): [Summarize relevant family history if discussed.]\n",
    "    Review of Systems (ROS): [Briefly list any systems reviewed and any symptoms reported by the patient related to those systems. Focus on relevant systems based on the chief complaint.]\n",
    "\n",
    "O - Objective:\n",
    "    Vital Signs: [List any vital signs mentioned in the transcription (BP, HR, RR, Temp, SpO2, Pain Scale) and their values if provided. If not explicitly stated in the transcription, state \"Not documented in transcription.\"]\n",
    "    Physical Exam Findings: [Summarize any physical exam findings described by the provider. Focus on findings related to the chief complaint and ROS. If no physical exam findings are explicitly mentioned in the transcription, state \"Not documented in transcription, infer from provider statements if possible (e.g., 'lungs sound clear' implies auscultation).\"]\n",
    "    Lab Results: [List any lab results mentioned by the provider or patient, including test name and result. If no lab results are mentioned, state \"Not documented in transcription.\"]\n",
    "    Imaging Results: [List any imaging results mentioned, including type and findings. If none mentioned, state \"Not documented in transcription.\"]\n",
    "    Other Diagnostic Tests: [List any other diagnostic test results mentioned (e.g., EKG, PFTs). If none mentioned, state \"Not documented in transcription.\"]\n",
    "\n",
    "A - Assessment:\n",
    "    Differential Diagnoses: [List any differential diagnoses discussed by the provider. Include potential diagnoses considered.]\n",
    "    Working Diagnosis (or Most Likely Diagnosis): [Identify the most likely diagnosis or working diagnosis stated or strongly implied by the provider. If no clear diagnosis is stated, summarize the provider's assessment of the patient's condition.]\n",
    "    Problem List: [List any active or chronic problems identified or confirmed by the provider during the encounter. Focus on problems relevant to this visit.]\n",
    "\n",
    "P - Plan:\n",
    "    Diagnostic Plan: [List any further diagnostic tests, labs, or imaging ordered or planned by the provider.]\n",
    "    Therapeutic Plan: [Summarize the treatment plan, including medications prescribed, procedures planned, therapies recommended, lifestyle modifications advised, and referrals made.]\n",
    "    Patient Education: [Summarize any patient education provided by the provider, including instructions, self-care advice, and information about medications or conditions.]\n",
    "    Follow-up Plan: [Describe the follow-up plan, including when the patient should return, specific instructions for follow-up, and any \"return precautions\" mentioned (e.g., \"return if symptoms worsen\").]\n",
    "    Consults/Referrals: [List any consultations or referrals to specialists or other providers planned by the provider.]   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.859315Z",
     "iopub.status.busy": "2025-04-19T21:16:37.858988Z",
     "iopub.status.idle": "2025-04-19T21:16:37.882470Z",
     "shell.execute_reply": "2025-04-19T21:16:37.881604Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.859285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_soap_note_from_audio(state: AgentState):\n",
    "    \"\"\"\n",
    "    Uses generative ai to summarize the audio  file to a soap note.\n",
    "    \"\"\"\n",
    "    audio_file = state.get(\"uploaded_audio_file\")\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[audio_file],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1,\n",
    "            system_instruction=soap_note_generation_sys_prompt\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\"soap_note\": response.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.883834Z",
     "iopub.status.busy": "2025-04-19T21:16:37.883481Z",
     "iopub.status.idle": "2025-04-19T21:16:37.908883Z",
     "shell.execute_reply": "2025-04-19T21:16:37.907926Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.883812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def truncate_text(text: str, n: int = 200) -> str:\n",
    "    \"\"\"Truncates text to the first n characters, adding ellipsis if truncated.\"\"\"    \n",
    "    if len(text) > n:\n",
    "        return text[:n] + \"...\"\n",
    "    return text\n",
    "\n",
    "def write_response(agent_state: AgentState):\n",
    "    \"\"\"\n",
    "    Pretty prints specific fields from the AgentState dictionary:\n",
    "    `quest`, `quest_resp`, `soap_note`, `soap_fhir_resources`.\n",
    "\n",
    "    Prints an error message if quest, quest_resp, and soap_note are not defined.\n",
    "    Uses section headers, delimiters, line spacing, and pretty printing for dictionaries and lists.\n",
    "    Prints each SOAP FHIR resource explicitly with an index.\n",
    "    \"\"\"\n",
    "\n",
    "    if not agent_state.get('quest') or not agent_state.get('quest_resp') or not agent_state.get('soap_note'):\n",
    "        print(\"=\" * 20)\n",
    "        print(\"Error: Cannot pretty print AgentState fields.\")\n",
    "        if not agent_state.get('quest'):\n",
    "            print(\"- 'quest' field is missing.\")\n",
    "        if not agent_state.get('quest_resp'):\n",
    "            print(\"- 'quest_resp' field is missing.\")\n",
    "        if not agent_state.get('soap_note'):\n",
    "            print(\"- 'soap_note' field is missing.\")\n",
    "        print(\"=\" * 20)\n",
    "        return\n",
    "\n",
    "    delimiter = \"=\" * 20 + \"\\n\"\n",
    "\n",
    "    print(delimiter)\n",
    "    print(\"======== Quest ========\")\n",
    "    print(delimiter)\n",
    "    print(truncate_text(json.dumps(agent_state['quest'], indent=2)))\n",
    "    print(\"\\n\" + delimiter)\n",
    "\n",
    "    print(\"===== Quest Response =====\")\n",
    "    print(delimiter)\n",
    "    print(truncate_text(json.dumps(agent_state['quest_resp'], indent=2)))\n",
    "    print(\"\\n\" + delimiter)\n",
    "\n",
    "    print(\"======== SOAP Note ========\")\n",
    "    print(delimiter)\n",
    "    print(truncate_text(agent_state['soap_note']))\n",
    "    print(\"\\n\" + delimiter)\n",
    "\n",
    "    print(\"==== SOAP FHIR Resources ====\")\n",
    "    print(delimiter)\n",
    "    if agent_state.get('soap_fhir_resources'):\n",
    "        for fhir_resource in agent_state['soap_fhir_resources']:\n",
    "            print(f\"\"\"--- SOAP FHIR Resource Index: {fhir_resource.get(\"resourceType\")} ---\"\"\")\n",
    "            print(truncate_text(json.dumps(fhir_resource, indent=2)))\n",
    "            print(\"-\" * 20 + \"\\n\") # Sub-delimiter for each resource\n",
    "    else:\n",
    "        print(\"No SOAP FHIR Resources found.\")\n",
    "    print(delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.910088Z",
     "iopub.status.busy": "2025-04-19T21:16:37.909760Z",
     "iopub.status.idle": "2025-04-19T21:16:37.935604Z",
     "shell.execute_reply": "2025-04-19T21:16:37.934765Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.910062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import base64\n",
    "\n",
    "def generate_binary_for_soap(soap_note):\n",
    "    \"\"\"\n",
    "    Generates binary resource that will store the soap note on the fhir servers\n",
    "    \"\"\"\n",
    "    soap_note_bytes = soap_note.encode('utf-8')\n",
    "    base64_encoded_content = base64.b64encode(soap_note_bytes).decode('utf-8')\n",
    "    binary = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"resourceType\": \"Binary\",\n",
    "        \"contentType\": \"text/plain\",\n",
    "        \"content\": base64_encoded_content\n",
    "    }\n",
    "\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.937132Z",
     "iopub.status.busy": "2025-04-19T21:16:37.936835Z",
     "iopub.status.idle": "2025-04-19T21:16:37.960450Z",
     "shell.execute_reply": "2025-04-19T21:16:37.959630Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.937109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json_repair\n",
    "\n",
    "def generate_doc_ref_for_soap(soap_note):\n",
    "    prompt = f\"\"\"You are an expert in FHIR (Fast Healthcare Interoperability Resources) and are tasked with creating a DocumentReference resource. This resource will serve as an index entry for a SOAP note that will be stored separately as a FHIR Binary resource.  You are given the content of the SOAP note as text.\n",
    "\n",
    "    Generate a well-formed JSON representation of a FHIR DocumentReference resource. This resource should contain metadata extracted from the SOAP note and information needed to locate the SOAP note once it is stored as a Binary resource.\n",
    "\n",
    "    **Thought Process:**\n",
    "\n",
    "    1. **Understand the Request:** I need to create a DocumentReference. This DocumentReference is *about* a SOAP note, not the SOAP note itself. The SOAP note will become a Binary resource later.\n",
    "\n",
    "    2. **Identify Key FHIR DocumentReference Fields:**  To properly index a SOAP note, I need to consider the essential fields in a DocumentReference.  Let's think about what information is crucial for indexing and retrieval:\n",
    "        * `resourceType`: Always \"DocumentReference\".\n",
    "        * `status`:  Likely \"current\".\n",
    "        * `subject`: Who is the SOAP note about? (Patient) - Extract from SOAP note.\n",
    "        * `date`: When was the SOAP note created? - Extract from SOAP note (or use current date if not found).\n",
    "        * `author`: Who wrote the SOAP note? (Practitioner/Provider) - Extract from SOAP note.\n",
    "        * `type`: What kind of document is this? (SOAP Note, Progress Note, etc.) - Needs to be coded.\n",
    "        * `category`:  Broader categories for the document (Clinical Note, etc.) - Needs to be coded.\n",
    "        * `content`:  Crucially, this describes the *content* being referenced. Since the SOAP note will be a Binary, we need:\n",
    "            * `content.attachment.contentType`:  What kind of content is the SOAP note? (e.g., text/plain, application/pdf if we assume it *could* be converted later). Let's assume text/plain for now since the input is text.\n",
    "            * `content.attachment.url`:  This is where we would point to the Binary resource *if it existed*. Since we aren't creating the Binary, we'll use a placeholder URI format that indicates it's a reference to a Binary. Something like `urn:uuid:{{binary-resource-id}}` would work, where `{{binary-resource-id}}` is a placeholder for a future Binary resource ID.\n",
    "            * `content.format`:  Describe the format of the SOAP note content itself (e.g., text/plain). We could also use a more specific coding if we know the format in more detail.\n",
    "        * `context`:  Contextual information, like the encounter or practice setting.  Less critical for basic indexing, but good to consider if information is available.  Could be Encounter reference.\n",
    "\n",
    "    3. **Information Extraction Strategy from SOAP Note:**  I need to look at the SOAP note text and identify the pieces of information needed for the DocumentReference fields (Patient, Author, Date, Document Type).  The format of the SOAP note might be somewhat structured even if it's just text. I'll need to make reasonable assumptions about common SOAP note structures. If information is missing or unclear, I should leave out the corresponding FHIR field.\n",
    "\n",
    "    4. **FHIR Coding for `type` and `category`:** I need to use CodeableConcepts for `type` and `category`. For SOAP notes, we can use LOINC codes for `type` (e.g., \"Progress note\" `72517-5`) and potentially SNOMED CT for `category` (e.g., \"Clinical note\" `114884008`). I will use LOINC for type and a general category.\n",
    "\n",
    "    Use this json schema for output:\n",
    "\n",
    "    return {{DocumentReference}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[prompt, soap_note],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            response_mime_type=\"application/json\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    document_ref = json_repair.loads(response.text)\n",
    "    return document_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.961662Z",
     "iopub.status.busy": "2025-04-19T21:16:37.961343Z",
     "iopub.status.idle": "2025-04-19T21:16:37.985166Z",
     "shell.execute_reply": "2025-04-19T21:16:37.984115Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.961639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_soap_resources(state: AgentState):\n",
    "    \"\"\"\n",
    "    create a document Reference resource to index a binary resource that holds the soap_note.\n",
    "    \"\"\"\n",
    "    soap_note = state.get(\"soap_note\")\n",
    "\n",
    "    binary = generate_binary_for_soap(soap_note)\n",
    "    document_ref = generate_doc_ref_for_soap(soap_note)\n",
    "    patient_id = state.get(\"patient_res\").get(\"id\")\n",
    "    practitioner_id = state.get(\"practitioner_res\").get(\"id\")\n",
    "    patient_ref = f\"Patient/{patient_id}\"\n",
    "    practitioner_ref = f\"Practitioner/{practitioner_id}\"\n",
    "    binary_id = binary.get(\"id\")\n",
    "\n",
    "    document_ref[\"content\"] = [\n",
    "        {\n",
    "            \"attachment\": {\n",
    "                \"contentType\": \"text/plain\",\n",
    "                \"url\": f\"Binary/{binary_id}\",  # Placeholder URL\n",
    "            },\n",
    "            \"format\": {\n",
    "                \"system\": \"http://terminology.hl7.org/CodeSystem/fhir-format-codes\",\n",
    "                \"code\": \"text/plain\",\n",
    "                \"display\": \"Plain text\",\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    document_ref[\"subject\"] = {\n",
    "        \"reference\": patient_ref\n",
    "    }\n",
    "    document_ref[\"author\"] = {\n",
    "        \"reference\": practitioner_ref\n",
    "    }\n",
    "    return {\"soap_fhir_resources\": [binary, document_ref]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Defining the graph workflow\n",
    "\n",
    "Bring every together, the nodes and create the edge connections that effect the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:37.987150Z",
     "iopub.status.busy": "2025-04-19T21:16:37.986302Z",
     "iopub.status.idle": "2025-04-19T21:16:38.432823Z",
     "shell.execute_reply": "2025-04-19T21:16:38.431374Z",
     "shell.execute_reply.started": "2025-04-19T21:16:37.987124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=model_id, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Defined the graph\n",
    "wk_graph = StateGraph(AgentState)\n",
    "\n",
    "def aggregate_state(state: AgentState):\n",
    "    return state\n",
    "\n",
    "def aggregate_state2(state: AgentState):\n",
    "    return state\n",
    "\n",
    "# node magic strings\n",
    "audio_file_upload_key = \"upload_file_to_gemini\"\n",
    "fetch_questionnaire_key = \"discover_and_fetch_questionnaire\"\n",
    "gen_quest_resp_key = \"generate_questionnaire_response\"\n",
    "gen_soap_note_key = \"generate_soap_note\"\n",
    "write_resp_key = \"write_resp\"\n",
    "quest_audio_state_aggregator_key = \"inputs_aggregator\"\n",
    "soap_quest_state_aggregator_key = \"output_aggregator\"\n",
    "generate_soap_resources_key=\"generate_soap_resources\"\n",
    "\n",
    "# Nodes\n",
    "wk_graph.add_node(audio_file_upload_key, upload_to_gemini)\n",
    "wk_graph.add_node(fetch_questionnaire_key, fetch_questionnaire)\n",
    "wk_graph.add_node(gen_quest_resp_key, get_questionnaire_response)\n",
    "wk_graph.add_node(gen_soap_note_key, generate_soap_note_from_audio)\n",
    "wk_graph.add_node(write_resp_key, write_response)\n",
    "wk_graph.add_node(quest_audio_state_aggregator_key, aggregate_state)\n",
    "wk_graph.add_node(soap_quest_state_aggregator_key, aggregate_state2)\n",
    "wk_graph.add_node(generate_soap_resources_key, generate_soap_resources)\n",
    "\n",
    "def check_file_upload(state: AgentState):\n",
    "    next_nodes = []\n",
    "    if state.get(\"uploaded_audio_file\") is None:\n",
    "        return write_resp_key\n",
    "    else:\n",
    "        next_nodes.append(gen_soap_note_key)\n",
    "    if state.get(\"quest_found\"):\n",
    "        next_nodes.append(gen_quest_resp_key)\n",
    "    return next_nodes\n",
    "\n",
    "\n",
    "# Edges\n",
    "wk_graph.add_edge(START, audio_file_upload_key)\n",
    "wk_graph.add_edge(START, fetch_questionnaire_key)\n",
    "wk_graph.add_edge(\n",
    "    [audio_file_upload_key, fetch_questionnaire_key],\n",
    "    quest_audio_state_aggregator_key)\n",
    "   \n",
    "wk_graph.add_conditional_edges(quest_audio_state_aggregator_key, check_file_upload)\n",
    "wk_graph.add_edge(gen_soap_note_key, generate_soap_resources_key)\n",
    "\n",
    "# instead of two add_edge calls, do:\n",
    "wk_graph.add_edge(\n",
    "    [gen_quest_resp_key, generate_soap_resources_key],\n",
    "    soap_quest_state_aggregator_key\n",
    ")\n",
    "wk_graph.add_edge(generate_soap_resources_key, soap_quest_state_aggregator_key)\n",
    "\n",
    "wk_graph.add_edge(soap_quest_state_aggregator_key, write_resp_key)\n",
    "\n",
    "wk_graph.add_edge(write_resp_key, END)\n",
    "\n",
    "graph = wk_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:38.434437Z",
     "iopub.status.busy": "2025-04-19T21:16:38.433996Z",
     "iopub.status.idle": "2025-04-19T21:16:38.586684Z",
     "shell.execute_reply": "2025-04-19T21:16:38.585724Z",
     "shell.execute_reply.started": "2025-04-19T21:16:38.434402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 Run the workflow\n",
    "\n",
    "The patient resources and practitioner resource would respectively represent the patient and provider in the emr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T21:16:38.590281Z",
     "iopub.status.busy": "2025-04-19T21:16:38.590014Z",
     "iopub.status.idle": "2025-04-19T21:16:50.483300Z",
     "shell.execute_reply": "2025-04-19T21:16:50.482393Z",
     "shell.execute_reply.started": "2025-04-19T21:16:38.590258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# the sample audio file that will be used\n",
    "audio_file_path = \"../Data/CAR0001.mp3\"\n",
    "\n",
    "practitioner_resource = {\n",
    "    \"resourceType\": \"Practitioner\",\n",
    "    \"id\": \"practitioner-adam-careful\",\n",
    "    \"identifier\": [{\"system\": \"http://hl7.org/fhir/sid/us-npi\", \"value\": \"9999999999\"}],\n",
    "    \"active\": True,\n",
    "    \"name\": [{\"family\": \"Careful\", \"given\": [\"Adam\"]}],\n",
    "}\n",
    "\n",
    "patient_resource = {\n",
    "    \"resourceType\": \"Patient\",\n",
    "    \"id\": \"patient-jane-doe\",\n",
    "    \"identifier\": [\n",
    "        {\"use\": \"usual\", \"system\": \"urn:oid:1.2.3.4.5.6.7\", \"value\": \"MRN12345\"}\n",
    "    ],\n",
    "    \"active\": True,\n",
    "    \"name\": [{\"use\": \"official\", \"family\": \"Doe\", \"given\": [\"Jane\"]}],\n",
    "    \"gender\": \"female\",\n",
    "    \"birthDate\": \"1985-08-15\",\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "    \"audio_file_path\": audio_file_path,\n",
    "    \"instructions\": \"Fill out a medical history report\",\n",
    "    \"practitioner_res\": practitioner_resource,\n",
    "    \"patient_res\": patient_resource\n",
    "}\n",
    "state = graph.invoke(inputs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7164736,
     "sourceId": 11473607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7114367,
     "sourceId": 11474026,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
