{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Agent Workflow Notebook\n",
    "\n",
    "This notebook implements an AI agent workflow that auto-fills FHIR forms (Questionnaires) from audio recordings. The solution uses a Retrieval-Augmented Generation (RAG) approach to discover the best matching Questionnaire and then generates a FHIR `QuestionnaireResponse`. \n",
    "\n",
    "We provide two solutions for orchestrating the workflow as an agent:\n",
    "\n",
    "- **Solution 1:** Uses LangChain and LangGraph to manage the task flow.\n",
    "- **Solution 2:** Uses direct integrations with Gemini (`gemini-2.0-flash`) and Vertex AI APIs for full orchestration.\n",
    "\n",
    "The HAPI FHIR server is used as our FHIR backend: `https://hapi.fhir.org/baseR4/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Install the required libraries before running the notebook. (Uncomment the pip install commands if needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run these if needed\n",
    "# !pip install requests nbformat chromadb langchain langgraph repair-json\n",
    "# Additional libraries might be needed for integration with Gemini/Vertex\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# For our vector DB, we'll use a simple in-memory structure to simulate the embeddings/indexing using Chroma's API\n",
    "from typing import List, Dict\n",
    "\n",
    "## Dummy embedding function (replace with real API call, e.g., OpenAI embeddings)\n",
    "def compute_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Computes a vector embedding for a given text.\n",
    "    Replace this with a call to an embedding API like OpenAI or HuggingFace.\n",
    "    \"\"\"\n",
    "    # For simplicity, use character ordinal values normalized (this is a dummy placeholder!)\n",
    "    return [float(ord(c)) for c in text[:50]]  # limiting length for demo\n",
    "\n",
    "## Simple in-memory vector store for demonstration purposes\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.index = []  # list of tuples: (questionnaire_id, embedding, metadata)\n",
    "\n",
    "    def add(self, questionnaire_id: str, embedding: List[float], metadata: Dict):\n",
    "        self.index.append((questionnaire_id, embedding, metadata))\n",
    "\n",
    "    def search(self, query_embedding: List[float], top_k: int = 1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform a dummy similarity search by computing Euclidean distance.\n",
    "        \"\"\"\n",
    "        def distance(vec1, vec2):\n",
    "            return sum((a - b) ** 2 for a, b in zip(vec1, vec2)) ** 0.5\n",
    "\n",
    "        scored = []\n",
    "        for q_id, emb, metadata in self.index:\n",
    "            d = distance(query_embedding, emb)\n",
    "            scored.append((d, q_id, metadata))\n",
    "        scored.sort(key=lambda x: x[0])\n",
    "        results = []\n",
    "        for score, q_id, metadata in scored[:top_k]:\n",
    "            results.append({\"questionnaire_id\": q_id, \"score\": score, \"metadata\": metadata})\n",
    "        return results\n",
    "\n",
    "# Initialize our in-memory vector store\n",
    "vector_store = VectorStore()\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Setup: Pull and Index FHIR Questionnaires\n",
    "\n",
    "This cell fetches all Questionnaire resources from the HAPI FHIR server, extracts key metadata, computes embeddings, and indexes them into our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FHIR_BASE = \"https://hapi.fhir.org/baseR4\"\n",
    "\n",
    "def fetch_questionnaires() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches all Questionnaire resources from the HAPI FHIR server.\n",
    "    \"\"\"\n",
    "    url = f\"{FHIR_BASE}/Questionnaire?_count=100\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch questionnaires: {response.text}\")\n",
    "    data = response.json()\n",
    "    return data.get('entry', [])\n",
    "\n",
    "def extract_metadata(questionnaire: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Extracts identifying information from a Questionnaire resource.\n",
    "    \"\"\"\n",
    "    resource = questionnaire.get('resource', {})\n",
    "    title = resource.get('title', \"\")\n",
    "    description = resource.get('description', \"\")\n",
    "    # Combine title and description for embedding\n",
    "    return f\"Title: {title}. Description: {description}\".strip()\n",
    "\n",
    "def index_questionnaires():\n",
    "    questionnaires = fetch_questionnaires()\n",
    "    print(f\"Fetched {len(questionnaires)} questionnaires\")\n",
    "    for entry in questionnaires:\n",
    "        resource = entry.get('resource', {})\n",
    "        q_id = resource.get('id')\n",
    "        metadata_str = extract_metadata(entry)\n",
    "        emb = compute_embedding(metadata_str)\n",
    "        vector_store.add(q_id, emb, {\"metadata\": metadata_str})\n",
    "    print(\"Indexing complete.\")\n",
    "\n",
    "# Run indexing\n",
    "index_questionnaires()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transcribe and Diarize Audio\n",
    "\n",
    "This function takes an audio file (path) and returns a transcription and speaker segments. (Replace the placeholder logic with actual model calls, e.g., Whisper.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_and_diarize(audio_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Transcribes and diarizes the input audio file.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'transcription' and 'speaker_segments'.\n",
    "    \"\"\"\n",
    "    # TODO: Replace with actual audio processing using, e.g., Whisper\n",
    "    transcription = \"This is a sample transcription from the audio file.\"\n",
    "    speaker_segments = [\"Speaker 1\", \"Speaker 2\"]\n",
    "    return {\"transcription\": transcription, \"speaker_segments\": speaker_segments}\n",
    "\n",
    "# Example usage:\n",
    "audio_data = transcribe_and_diarize(\"path/to/audio.wav\")\n",
    "print(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discover Relevant Questionnaire via RAG Retrieval\n",
    "\n",
    "This function uses a user prompt to compute its embedding and performs a vector search against our indexed Questionnaires. It then retrieves the best matching Questionnaire from the HAPI FHIR server. If no relevant Questionnaire is found, the function raises an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_questionnaire(prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Discovers the most relevant Questionnaire from the pre-indexed embeddings using a RAG approach.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The user text input describing the desired form.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The Questionnaire resource in JSON format.\n",
    "              If no relevant questionnaire is found, raises an Exception.\n",
    "    \"\"\"\n",
    "    # Compute embedding for the prompt\n",
    "    prompt_embedding = compute_embedding(prompt)\n",
    "    # Search our in-memory vector store\n",
    "    results = vector_store.search(prompt_embedding, top_k=1)\n",
    "    if not results:\n",
    "        raise Exception(\"No matching questionnaire found for the provided prompt.\")\n",
    "    best_match = results[0]\n",
    "    questionnaire_id = best_match[\"questionnaire_id\"]\n",
    "\n",
    "    # Retrieve the full Questionnaire from the HAPI FHIR server\n",
    "    url = f\"{FHIR_BASE}/Questionnaire/{questionnaire_id}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve Questionnaire {questionnaire_id}: {response.text}\")\n",
    "    questionnaire = response.json()\n",
    "    return questionnaire\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    questionnaire = discover_questionnaire(\"Record patient vitals\")\n",
    "    print(\"Questionnaire found:\", questionnaire.get('id'))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate QuestionnaireResponse\n",
    "\n",
    "This function generates a FHIR `QuestionnaireResponse` from the transcription and the retrieved Questionnaire. In a production system, this would likely use a generative model such as the `gemini-2.0-flash` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questionnaire_response(transcription: str, questionnaire: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a FHIR QuestionnaireResponse based on the transcription and the retrieved Questionnaire.\n",
    "    \n",
    "    Args:\n",
    "        transcription (str): The transcribed text from the audio.\n",
    "        questionnaire (dict): The Questionnaire resource JSON.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A generated QuestionnaireResponse resource (dummy implementation here).\n",
    "    \"\"\"\n",
    "    # TODO: Replace with a call to a generative model (e.g., gemini-2.0-flash) if needed\n",
    "    response = {\n",
    "        \"resourceType\": \"QuestionnaireResponse\",\n",
    "        \"questionnaire\": questionnaire.get('id', 'unknown'),\n",
    "        \"status\": \"completed\",\n",
    "        \"text\": transcription,\n",
    "        \"item\": []  # Populate with actual answers based on questionnaire items\n",
    "    }\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "dummy_q = {\"id\": \"example-id\"}\n",
    "q_response = generate_questionnaire_response(audio_data[\"transcription\"], dummy_q)\n",
    "print(q_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Repair and Validate QuestionnaireResponse\n",
    "\n",
    "This function repairs any malformed JSON (using a placeholder for `repair_json`) and validates the QuestionnaireResponse against the HAPI FHIR server using the `$validate` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_and_validate_response(questionnaire_response: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Repairs and validates the QuestionnaireResponse against the HAPI FHIR server.\n",
    "    \n",
    "    Args:\n",
    "        questionnaire_response (dict): The QuestionnaireResponse resource.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The result of the validation.\n",
    "        If validation fails, raises an Exception.\n",
    "    \"\"\"\n",
    "    # Dummy repair step: In practice, you might call repair_json here\n",
    "    repaired_response = questionnaire_response  # Assuming it's correct for this demo\n",
    "\n",
    "    # Validate against HAPI FHIR using the $validate operation\n",
    "    validate_url = f\"{FHIR_BASE}/QuestionnaireResponse/$validate\"\n",
    "    headers = {\"Content-Type\": \"application/fhir+json\"}\n",
    "    response = requests.post(validate_url, headers=headers, data=json.dumps(repaired_response))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Validation failed: {response.text}\")\n",
    "    validation_result = response.json()\n",
    "    return validation_result\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    validation = repair_and_validate_response(q_response)\n",
    "    print(\"Validation successful:\", validation)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save QuestionnaireResponse\n",
    "\n",
    "This function saves the validated QuestionnaireResponse to the HAPI FHIR server using a POST request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_questionnaire_response(questionnaire_response: dict) -> str:\n",
    "    \"\"\"\n",
    "    Saves the QuestionnaireResponse resource to the HAPI FHIR server.\n",
    "    \n",
    "    Args:\n",
    "        questionnaire_response (dict): The validated QuestionnaireResponse resource.\n",
    "    \n",
    "    Returns:\n",
    "        str: A confirmation message if saving is successful.\n",
    "    \"\"\"\n",
    "    url = f\"{FHIR_BASE}/QuestionnaireResponse\"\n",
    "    headers = {\"Content-Type\": \"application/fhir+json\"}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(questionnaire_response))\n",
    "    if response.status_code not in [200, 201]:\n",
    "        raise Exception(f\"Failed to save QuestionnaireResponse: {response.text}\")\n",
    "    return \"QuestionnaireResponse saved successfully.\"\n",
    "\n",
    "# Example usage:\n",
    "try:\n",
    "    save_message = save_questionnaire_response(q_response)\n",
    "    print(save_message)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent Orchestration\n",
    "\n",
    "We implement two approaches for orchestrating the end-to-end workflow as an agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Agent Orchestration using LangChain & LangGraph\n",
    "\n",
    "In this approach, we encapsulate each workflow step as an individual chain/node, and a LangChain agent coordinates the flow. (This is a placeholder implementation to demonstrate the design.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "def langchain_agent_workflow(audio_path: str, prompt: str):\n",
    "    \"\"\"\n",
    "    Orchestrates the workflow using a LangChain agent.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): The path to the audio file.\n",
    "        prompt (str): The user prompt for questionnaire discovery.\n",
    "    \n",
    "    Returns:\n",
    "        str: Final confirmation message after the workflow completes.\n",
    "    \"\"\"\n",
    "    # Step 1: Transcribe & Diarize\n",
    "    audio_data = transcribe_and_diarize(audio_path)\n",
    "\n",
    "    # Step 2: Discover Questionnaire via RAG Retrieval\n",
    "    questionnaire = discover_questionnaire(prompt)\n",
    "\n",
    "    # Step 3: Generate QuestionnaireResponse\n",
    "    q_response = generate_questionnaire_response(audio_data[\"transcription\"], questionnaire)\n",
    "\n",
    "    # Step 4: Repair and Validate\n",
    "    repair_and_validate_response(q_response)  # will raise exception if invalid\n",
    "\n",
    "    # Step 5: Save the QuestionnaireResponse\n",
    "    result = save_questionnaire_response(q_response)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage of LangChain Agent\n",
    "try:\n",
    "    result_msg = langchain_agent_workflow(\"path/to/audio.wav\", \"Record patient vitals\")\n",
    "    print(\"LangChain Agent Workflow Result:\", result_msg)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Agent Orchestration using Gemini + Vertex APIs\n",
    "\n",
    "In this approach, we directly call the Gemini (`gemini-2.0-flash`) model and Vertex AI APIs to orchestrate the workflow. \n",
    "\n",
    "Note: In this placeholder implementation, we simulate API calls with dummy functions. Replace these with actual API integration as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy functions to simulate Gemini and Vertex API calls\n",
    "def call_gemini_model(task: str, payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Simulates a call to the gemini-2.0-flash model via an API.\n",
    "    \"\"\"\n",
    "    # Placeholder response: in practice, send an API request\n",
    "    print(f\"Calling Gemini for task '{task}' with payload: {payload}\")\n",
    "    return {\"result\": f\"Gemini result for {task}\"}\n",
    "\n",
    "def call_vertex_api(task: str, payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Simulates a call to a Vertex AI endpoint.\n",
    "    \"\"\"\n",
    "    print(f\"Calling Vertex API for task '{task}' with payload: {payload}\")\n",
    "    return {\"result\": f\"Vertex result for {task}\"}\n",
    "\n",
    "def gemini_vertex_agent_workflow(audio_path: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Orchestrates the workflow by directly calling Gemini and Vertex AI APIs.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): The path to the audio file.\n",
    "        prompt (str): The user prompt for questionnaire discovery.\n",
    "    \n",
    "    Returns:\n",
    "        str: Final confirmation message after the workflow completes.\n",
    "    \"\"\"\n",
    "    # Step 1: Transcribe & Diarize via Gemini\n",
    "    gemini_transcription = call_gemini_model(\"transcribe_and_diarize\", {\"audio_path\": audio_path})\n",
    "    # Simulate extraction of transcription\n",
    "    transcription = \"This is the transcription from Gemini.\"\n",
    "\n",
    "    # Step 2: Discover Questionnaire via RAG (simulate using Vertex API for retrieval)\n",
    "    gemini_prompt = {\"prompt\": prompt}\n",
    "    vertex_result = call_vertex_api(\"discover_questionnaire\", gemini_prompt)\n",
    "    # For demo, retrieve a dummy Questionnaire\n",
    "    questionnaire = {\"id\": \"example-id\", \"resourceType\": \"Questionnaire\"}\n",
    "\n",
    "    # Step 3: Generate QuestionnaireResponse via Gemini\n",
    "    gen_payload = {\"transcription\": transcription, \"questionnaire\": questionnaire}\n",
    "    gemini_gen = call_gemini_model(\"generate_questionnaire_response\", gen_payload)\n",
    "    q_response = {\n",
    "        \"resourceType\": \"QuestionnaireResponse\",\n",
    "        \"questionnaire\": questionnaire.get(\"id\"),\n",
    "        \"status\": \"completed\",\n",
    "        \"text\": transcription,\n",
    "        \"item\": []\n",
    "    }\n",
    "\n",
    "    # Step 4: Repair and Validate via Vertex API\n",
    "    repair_payload = {\"questionnaire_response\": q_response}\n",
    "    vertex_validation = call_vertex_api(\"repair_and_validate_response\", repair_payload)\n",
    "    # Assuming validation passed\n",
    "\n",
    "    # Step 5: Save QuestionnaireResponse via Gemini\n",
    "    save_payload = {\"questionnaire_response\": q_response}\n",
    "    gemini_save = call_gemini_model(\"save_questionnaire_response\", save_payload)\n",
    "\n",
    "    return \"QuestionnaireResponse saved via Gemini+Vertex Agent Workflow.\"\n",
    "\n",
    "# Example usage of Gemini + Vertex API Agent\n",
    "try:\n",
    "    result_msg2 = gemini_vertex_agent_workflow(\"path/to/audio.wav\", \"Record patient vitals\")\n",
    "    print(\"Gemini+Vertex Agent Workflow Result:\", result_msg2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Orchestration Cell\n",
    "\n",
    "This cell demonstrates the full end-to-end orchestration of the workflow using both agent solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_workflow(audio_path: str, prompt: str):\n",
    "    \"\"\"\n",
    "    Main orchestration function that runs both solutions and compares results.\n",
    "    \"\"\"\n",
    "    print(\"Running LangChain & LangGraph Agent Workflow...\")\n",
    "    try:\n",
    "        result1 = langchain_agent_workflow(audio_path, prompt)\n",
    "        print(\"LangChain Agent Result:\", result1)\n",
    "    except Exception as e:\n",
    "        print(\"LangChain Agent failed:\", e)\n",
    "\n",
    "    print(\"\\nRunning Gemini + Vertex Agent Workflow...\")\n",
    "    try:\n",
    "        result2 = gemini_vertex_agent_workflow(audio_path, prompt)\n",
    "        print(\"Gemini+Vertex Agent Result:\", result2)\n",
    "    except Exception as e:\n",
    "        print(\"Gemini+Vertex Agent failed:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with actual paths/prompts\n",
    "    test_audio_path = \"path/to/audio.wav\"\n",
    "    test_prompt = \"Record patient vitals\"\n",
    "    main_workflow(test_audio_path, test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive blueprint for a FHIR Agent workflow that processes audio recordings, discovers relevant FHIR Questionnaires using a RAG-based approach, generates a QuestionnaireResponse, repairs/validates it, and then saves it to the HAPI FHIR server.\n",
    "\n",
    "Two agent orchestration solutions are demonstrated:\n",
    "\n",
    "- **LangChain & LangGraph Based Agent**: Utilizes modular chains to orchestrate the workflow.\n",
    "- **Gemini + Vertex APIs Based Agent**: Directly calls the Gemini (`gemini-2.0-flash`) and Vertex AI endpoints.\n",
    "\n",
    "Replace the placeholder sections with real API calls and integration logic as needed for a production system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
