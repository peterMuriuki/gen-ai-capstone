{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Clinical Documentation Assistant\n",
    "\n",
    "## Problem overview\n",
    "\n",
    "Healthcare professionals face a significant burden from medical documentation. This project focuses on leveraging generative AI to alleviate this burden by automatically extracting structured information from physician-patient audio conversations and using it to pre-fill administrative forms by generating data points that can be electronically stored in EMRs, and EHRS.\n",
    "\n",
    "We particularly focus on how the below presented workflow can be used to fill a medical history form an audio recording.\n",
    "\n",
    "This tool outputs data in a FHIR compatible format which ensures seamless integration with existing healthcare systems through a standardized, interoperable format. This structured approach unlocks the data's potential for reusability in various clinical workflows, analytics, and future healthcare applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution architecture\n",
    "\n",
    "This tool implements a RAG-based approach for form/[questionaire](https://www.hl7.org/fhir/R4/questionnaireresponse.html) discovery depending on a users prompt.\n",
    "\n",
    "It then generates a [QuestionnaireResponse](https://www.hl7.org/fhir/R4/questionnaireresponse.html), which represents an instance of a form submission\n",
    "\n",
    "In the place of FHIR compatible server we use a json file placeholder to act as a questionnaire repository\n",
    "\n",
    "[TODO - insert workflow image here preferably landscape]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:38:57.732190Z",
     "iopub.status.busy": "2025-04-16T21:38:57.731892Z",
     "iopub.status.idle": "2025-04-16T21:39:04.617568Z",
     "shell.execute_reply": "2025-04-16T21:39:04.616536Z",
     "shell.execute_reply.started": "2025-04-16T21:38:57.732166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n",
    "!pip install -q \"google-genai==1.7.0\" \"chromadb==0.6.3\" \"langchain==0.3.23\" \"langgraph==0.3.29\" \"json-repair==0.41.1\" \"google-api-core==2.24.2\" \"langchain-google-genai==2.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up your API key**\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:04.619634Z",
     "iopub.status.busy": "2025-04-16T21:39:04.619277Z",
     "iopub.status.idle": "2025-04-16T21:39:04.886122Z",
     "shell.execute_reply": "2025-04-16T21:39:04.885492Z",
     "shell.execute_reply.started": "2025-04-16T21:39:04.619608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyDAZjElfeaJqItRsB21v3p4ETShat1PzmI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:04.887238Z",
     "iopub.status.busy": "2025-04-16T21:39:04.886958Z",
     "iopub.status.idle": "2025-04-16T21:39:05.014460Z",
     "shell.execute_reply": "2025-04-16T21:39:05.013803Z",
     "shell.execute_reply.started": "2025-04-16T21:39:04.887212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import retry\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "model_id = \"gemini-2.0-flash\"\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "**Creating the embedding database with ChromaDB**\n",
    "\n",
    "We create a [custom function](https://docs.trychroma.com/guides/embeddings#custom-embedding-functions) to generate embeddings with the Gemini API. \n",
    "\n",
    "We will use this to store our questionnaire description as documents in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:05.015610Z",
     "iopub.status.busy": "2025-04-16T21:39:05.015291Z",
     "iopub.status.idle": "2025-04-16T21:39:05.021932Z",
     "shell.execute_reply": "2025-04-16T21:39:05.021056Z",
     "shell.execute_reply.started": "2025-04-16T21:39:05.015582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "# class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "#     # Specify whether to generate embeddings for documents, or queries\n",
    "#     document_mode = True\n",
    "\n",
    "#     @retry.Retry(predicate=is_retriable)\n",
    "#     def __call__(self, input: Documents) -> Embeddings:\n",
    "#         if self.document_mode:\n",
    "#             embedding_task = \"retrieval_document\"\n",
    "#         else:\n",
    "#             embedding_task = \"retrieval_query\"\n",
    "\n",
    "#         response = client.models.embed_content(\n",
    "#             model=\"models/text-embedding-004\",\n",
    "#             contents=input,\n",
    "#             config=types.EmbedContentConfig(\n",
    "#                 task_type=embedding_task,\n",
    "#             ),\n",
    "#         )\n",
    "#         return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a few utils that will be used to load the json questionnaire data. Ideally the data source would be a live FHIR server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:05.026683Z",
     "iopub.status.busy": "2025-04-16T21:39:05.026188Z",
     "iopub.status.idle": "2025-04-16T21:39:05.041417Z",
     "shell.execute_reply": "2025-04-16T21:39:05.040621Z",
     "shell.execute_reply.started": "2025-04-16T21:39:05.026661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# _quest_docs = None\n",
    "# def read_questionnaires_from_fs():\n",
    "#     global _quest_docs\n",
    "#     if _quest_docs is None:\n",
    "#         with open(\"/kaggle/input/quest-sample-db/quest.db.json\", \"r\") as file:\n",
    "#             _quest_docs = json.loads(file.read())\n",
    "#     return _quest_docs\n",
    "\n",
    "# def get_quest_docs_meta():\n",
    "#     quest_docs = read_questionnaires_from_fs()\n",
    "#     doc_with_metad = []\n",
    "#     doc_ids = []\n",
    "#     for doc in quest_docs:\n",
    "#         doc_id = doc.get(\"id\")\n",
    "#         doc_meta = {\n",
    "#             k: v\n",
    "#             for k, v in {\n",
    "#                 \"id\": doc_id,\n",
    "#                 \"title\": doc.get(\"title\"),\n",
    "#                 \"name\": doc.get(\"name\"),\n",
    "#             }.items()\n",
    "#             if v is not None\n",
    "#         }\n",
    "#         doc_desc = (\n",
    "#             doc.get(\"description\") if doc.get(\"description\") else \"No description\"\n",
    "#         )\n",
    "#         doc_with_metad.append((doc_desc, doc_meta))\n",
    "#         doc_ids.append(doc_id)\n",
    "#     return doc_with_metad, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load our questionnaire data and populate the vector database. we store the descriptions as vector embeddings and tag it with some metadata pertaining to each form/questionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:05.042566Z",
     "iopub.status.busy": "2025-04-16T21:39:05.042259Z",
     "iopub.status.idle": "2025-04-16T21:39:05.631820Z",
     "shell.execute_reply": "2025-04-16T21:39:05.630981Z",
     "shell.execute_reply.started": "2025-04-16T21:39:05.042539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "\n",
    "# DB_NAME = \"fhir-quest-semantic\"\n",
    "\n",
    "# embed_fn = GeminiEmbeddingFunction()\n",
    "# chroma_client = chromadb.Client()\n",
    "# db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "# def populate_vector_db():\n",
    "#     embed_fn.document_mode = True\n",
    "#     (desc_with_metad, doc_ids) = get_quest_docs_meta()\n",
    "#     descriptions, meta = zip(*desc_with_metad)\n",
    "#     print(meta)\n",
    "\n",
    "#     db.add(documents=list(descriptions), ids=doc_ids, metadatas=list(meta))\n",
    "\n",
    "# populate_vector_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the data was inserted by looking at the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval: Finding relevant questionnaires\n",
    "\n",
    "We will be using the user prompt to find a relevant questionnaire to fill. We do so by\n",
    "\n",
    "1. Querying our vector store for the single top most questionnaire that is semantically related to the users needs\n",
    "2. Use the gemini model to validate that the questionnaire does actually relate to the users prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:05.632951Z",
     "iopub.status.busy": "2025-04-16T21:39:05.632738Z",
     "iopub.status.idle": "2025-04-16T21:39:05.637771Z",
     "shell.execute_reply": "2025-04-16T21:39:05.637121Z",
     "shell.execute_reply.started": "2025-04-16T21:39:05.632934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def generate_form_validation_prompt(user_prompt, quest_desc, quest_metadata):\n",
    "#     return f\"\"\"\n",
    "# # Instruction\n",
    "# You are an evaluator. Your task is to evaluate the relevance of a form description and metadata to a user instruction.\n",
    "# We will provide you with the user instruction, and the form description and metadata.\n",
    "# Read the user instruction carefully to understand the user's need, and then evaluate if the provided form description and metadata are relevant to fulfilling that need based on the Criteria provided in the Evaluation section below.\n",
    "# You will assign the form description a rating following the Rating Rubric\n",
    "\n",
    "# # Evaluation\n",
    "# ## Metric Definition\n",
    "# You will be assessing form relevance, which measures whether the provided form description and metadata are suitable for fulfilling the user's instruction.  Relevance implies that a user could likely find the form useful and pertinent to their stated need.\n",
    "\n",
    "# ## Criteria\n",
    "# Relevance to User Instruction: The form description and metadata align with the user's instruction and suggest the form could potentially address the user's need.\n",
    "# Usefulness for User Instruction: The form, as described, appears practically useful for a user attempting to follow the given instruction.\n",
    "# Clarity of Description: The form description and metadata are clear and understandable enough to assess relevance. (If description is unclear, down-rate even if potentially relevant).\n",
    "\n",
    "# ## Rating Rubric\n",
    "# (YES). The form is very likely to be relevant and useful for the user instruction. The description is clear and strongly suggests a good match.\n",
    "# (NO). The form is not relevant to the user instruction. The description clearly indicates the form is unrelated to the user's need.\n",
    "\n",
    "# # User Inputs and Model Rating\n",
    "# ## User Instruction\n",
    "\n",
    "# ### Prompt\n",
    "# {user_prompt}\n",
    "\n",
    "# ## Form Description and Metadata\n",
    "\n",
    "# ### Form Instruction Description\n",
    "# {quest_desc}\n",
    "\n",
    "# ### Form Metadata (JSON)\n",
    "# {quest_metadata}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use JSON mode to control the models output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our agent's internal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:05.638802Z",
     "iopub.status.busy": "2025-04-16T21:39:05.638607Z",
     "iopub.status.idle": "2025-04-16T21:39:05.655302Z",
     "shell.execute_reply": "2025-04-16T21:39:05.654619Z",
     "shell.execute_reply.started": "2025-04-16T21:39:05.638787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import enum\n",
    "\n",
    "\n",
    "# class RelevantRating(enum.Enum):\n",
    "#     YES = \"Yes\"\n",
    "#     NO = \"No\"\n",
    "\n",
    "# def discover_questionnaire(query):\n",
    "#     embed_fn.document_mode = False\n",
    "#     result = db.query(query_texts=[query], n_results=1)\n",
    "#     queried_doc_ids = result.get(\"ids\")\n",
    "#     try:\n",
    "#         interest_doc_id = queried_doc_ids[0][0]\n",
    "#     except IndexError:\n",
    "#         return None\n",
    "#     queried_doc_desc = result.get(\"documents\")[0][0]\n",
    "#     queried_doc_meta = result.get(\"metadatas\")[0][0]\n",
    "#     prompt = generate_form_validation_prompt(query, queried_doc_desc, queried_doc_meta)\n",
    "#     print(\"PRompt\", prompt)\n",
    "\n",
    "#     structured_output_config = types.GenerateContentConfig(\n",
    "#         response_mime_type=\"text/x.enum\",\n",
    "#         response_schema=RelevantRating,\n",
    "#     )\n",
    "#     response = client.models.generate_content(\n",
    "#         model=model_id, contents=[prompt], config=structured_output_config\n",
    "#     )\n",
    "#     parsed_resp = response.parsed\n",
    "\n",
    "#     if parsed_resp is RelevantRating.YES:\n",
    "#         return interest_doc_id\n",
    "#     else:\n",
    "#         return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:05.657137Z",
     "iopub.status.busy": "2025-04-16T21:39:05.656276Z",
     "iopub.status.idle": "2025-04-16T21:39:09.929746Z",
     "shell.execute_reply": "2025-04-16T21:39:09.928824Z",
     "shell.execute_reply.started": "2025-04-16T21:39:05.657115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain==0.3.23\" \"langgraph==0.3.29\" \"json-repair==0.41.1\" \"langchain-google-genai==2.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:09.931755Z",
     "iopub.status.busy": "2025-04-16T21:39:09.930896Z",
     "iopub.status.idle": "2025-04-16T21:39:09.937214Z",
     "shell.execute_reply": "2025-04-16T21:39:09.936369Z",
     "shell.execute_reply.started": "2025-04-16T21:39:09.931713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Any, Dict, List\n",
    "\n",
    "# Define the state of our graph\n",
    "class AgentState(TypedDict):\n",
    "    audio_file_path: str\n",
    "    uploaded_audio_file: Any\n",
    "    # past_soap_notes: Dict[str, Any]\n",
    "    transcription: str\n",
    "    soap_note: str\n",
    "    fhir_resources: Any # TODO - verify typings are appropriate\n",
    "    clinical_intent: str\n",
    "    resource_plan: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:09.938254Z",
     "iopub.status.busy": "2025-04-16T21:39:09.938049Z",
     "iopub.status.idle": "2025-04-16T21:39:09.955719Z",
     "shell.execute_reply": "2025-04-16T21:39:09.954902Z",
     "shell.execute_reply.started": "2025-04-16T21:39:09.938238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def fetch_questionnaire(state: AgentState):\n",
    "#     query = state.get(\"instructions\")\n",
    "#     quest_id = discover_questionnaire(query)\n",
    "\n",
    "#     full_quest_docs = read_questionnaires_from_fs()\n",
    "#     of_interest_quest = None\n",
    "#     for quest in full_quest_docs:\n",
    "#         if quest[\"id\"] == quest_id:\n",
    "#             of_interest_quest = quest\n",
    "#             break\n",
    "#     if of_interest_quest is None:\n",
    "#         return {\"quest_found\": False}\n",
    "#     else:\n",
    "#         return {\"quest_found\": True, \"quest\": of_interest_quest}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_upload_file_cache = None\n",
    "\n",
    "def upload_to_gemini(state: AgentState):\n",
    "    \"\"\"\n",
    "    Uploads the local audio file to Gemini if not already uploaded.\n",
    "    Returns a dictionary with the uploaded file object.\n",
    "    \"\"\"\n",
    "    global _upload_file_cache\n",
    "    local_file_path = state.get(\"audio_file_path\")\n",
    "\n",
    "    try:\n",
    "        if _upload_file_cache is None:\n",
    "            _upload_file_cache = client.files.upload(file=local_file_path)\n",
    "        return {\"uploaded_audio_file\": _upload_file_cache}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to Gemini: {e}\")\n",
    "        # You can also return None, raise the error, or log it more formally\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_audio(state: AgentState):\n",
    "    prompt = \"\"\"\n",
    "        Diarize and transcribe this health-related interview, maintaining chronological order with timestamps if possible. Add labels for speaker (like 'Doctor:', 'Patient:', or 'Speaker 1:', 'Speaker 2:') at the beginning of each turn.\n",
    "        Accurately capture medical terms, mark unclear words as “[INAUDIBLE],” avoid adding extra commentary or guesses, and keep overlapping speech on separate lines. \n",
    "        Return only the final transcript.\n",
    "        \"\"\"\n",
    "    uploaded_audio_file = state.get(\"uploaded_audio_file\")\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id, contents=[prompt, uploaded_audio_file]\n",
    "    )\n",
    "\n",
    "    transcription = response.text.strip()\n",
    "    return {\"transcription\": transcription}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make make the call to the model with the questionniare that describes the form, and the audio file expecting the qeustionnaire response. The questionnaire response represents the form submission that a physician would have had to make manually from listening or partaking in the recorded conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:09.956866Z",
     "iopub.status.busy": "2025-04-16T21:39:09.956629Z",
     "iopub.status.idle": "2025-04-16T21:39:09.967410Z",
     "shell.execute_reply": "2025-04-16T21:39:09.966723Z",
     "shell.execute_reply.started": "2025-04-16T21:39:09.956848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from json_repair import repair_json\n",
    "\n",
    "# def get_questionnaire_response(state: AgentState):\n",
    "#     audio_file = state.get(\"uploaded_audio_file\")\n",
    "#     questionnaire = state.get(\"quest\")\n",
    "#     patient_emr = None # state.get(\"patient_emr\")\n",
    "#     # patient_emr, audio_file, questionnaire, questionnaireRes_example\n",
    "#     prompt = f\"\"\"\n",
    "#         You are an audio processing expert with extensive experience in converting audio files into structured data formats, specifically JSON. Your specialty lies in accurately extracting meaningful information from audio recordings and populating questionnaire-style data structures based on that information.\n",
    "        \n",
    "#         Your task is to analyze the provided audio file and patient Electronic Medical Record (EMR) and fill out the questionnaire with the relevant responses. The output format should follow the structure of the provided questionnaireResponse example.\n",
    "\n",
    "#         Here is the patient EMR:\n",
    "#         {patient_emr}\n",
    "        \n",
    "#         Here is the questionnaire:\n",
    "#         {questionnaire}\n",
    "        \n",
    "#         Here is an example questionnaire response:\n",
    "#         {questionnaireRes_example}\n",
    "        \n",
    "#         Please analyze the audio and generate the appropriate questionnaire response.\n",
    "\n",
    "#         Use this JSON schema:\n",
    "\n",
    "#         QuestionnaireResponse = <generated questionnaireResponse>\n",
    "#         return: QuestionnaireResponse\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.models.generate_content(\n",
    "#         model=model_id,\n",
    "#         contents=[prompt, audio_file],\n",
    "#         config=types.GenerateContentConfig(\n",
    "#             temperature=0,\n",
    "#             response_mime_type='application/json',\n",
    "#         )\n",
    "#     )\n",
    "#     qr = repair_json.loads(response.text)\n",
    "#     return {\"quest_resp\": qr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soap_note_generation_sys_prompt = \"\"\"You are an expert medical scribe tasked with generating a concise and accurate SOAP (Subjective, Objective, Assessment, Plan) note from a health care provider - patient conversation transcription.\n",
    "\n",
    "**Input:** You will be provided with a diarized transcription of a patient-provider encounter.  The transcription will clearly identify each speaker as either \"Patient\" or \"Provider\" at the beginning of each dialogue turn.\n",
    "\n",
    "**Task:**  Analyze the transcription and extract relevant information to populate each section of a SOAP note.\n",
    "\n",
    "**Output:**  Generate a SOAP note in the following structured format:\n",
    "\n",
    "S - Subjective:\n",
    "\n",
    "    Chief Complaint (CC): [Concise statement of the patient's primary reason for visit]\n",
    "\n",
    "    History of Present Illness (HPI): [Detailed narrative of the patient's current problem, using OLDCARTS or similar mnemonic if applicable. Include onset, location, duration, character, aggravating/alleviating factors, radiation, timing, severity.]\n",
    "\n",
    "    Past Medical History (PMH): [Summarize relevant past medical conditions mentioned by the patient or provider.]\n",
    "\n",
    "    Medications: [List current medications mentioned by the patient.]\n",
    "\n",
    "    Allergies: [List known allergies mentioned by the patient.]\n",
    "\n",
    "    Social History (SH): [Extract pertinent social history details like smoking, alcohol use, occupation, living situation if discussed and relevant to the encounter.]\n",
    "\n",
    "    Family History (FH): [Summarize relevant family history if discussed.]\n",
    "\n",
    "    Review of Systems (ROS): [Briefly list any systems reviewed and any symptoms reported by the patient related to those systems. Focus on relevant systems based on the chief complaint.]\n",
    "\n",
    "O - Objective:\n",
    "\n",
    "    Vital Signs: [List any vital signs mentioned in the transcription (BP, HR, RR, Temp, SpO2, Pain Scale) and their values if provided. If not explicitly stated in the transcription, state \"Not documented in transcription.\"]\n",
    "\n",
    "    Physical Exam Findings: [Summarize any physical exam findings described by the provider. Focus on findings related to the chief complaint and ROS. If no physical exam findings are explicitly mentioned in the transcription, state \"Not documented in transcription, infer from provider statements if possible (e.g., 'lungs sound clear' implies auscultation).\"]\n",
    "\n",
    "    Lab Results: [List any lab results mentioned by the provider or patient, including test name and result. If no lab results are mentioned, state \"Not documented in transcription.\"]\n",
    "\n",
    "    Imaging Results: [List any imaging results mentioned, including type and findings. If none mentioned, state \"Not documented in transcription.\"]\n",
    "\n",
    "    Other Diagnostic Tests: [List any other diagnostic test results mentioned (e.g., EKG, PFTs). If none mentioned, state \"Not documented in transcription.\"]\n",
    "\n",
    "A - Assessment:\n",
    "\n",
    "    Differential Diagnoses: [List any differential diagnoses discussed by the provider. Include potential diagnoses considered.]\n",
    "\n",
    "    Working Diagnosis (or Most Likely Diagnosis): [Identify the most likely diagnosis or working diagnosis stated or strongly implied by the provider. If no clear diagnosis is stated, summarize the provider's assessment of the patient's condition.]\n",
    "\n",
    "    Problem List: [List any active or chronic problems identified or confirmed by the provider during the encounter. Focus on problems relevant to this visit.]\n",
    "\n",
    "P - Plan:\n",
    "\n",
    "    Diagnostic Plan: [List any further diagnostic tests, labs, or imaging ordered or planned by the provider.]\n",
    "\n",
    "    Therapeutic Plan: [Summarize the treatment plan, including medications prescribed, procedures planned, therapies recommended, lifestyle modifications advised, and referrals made.]\n",
    "\n",
    "    Patient Education: [Summarize any patient education provided by the provider, including instructions, self-care advice, and information about medications or conditions.]\n",
    "\n",
    "    Follow-up Plan: [Describe the follow-up plan, including when the patient should return, specific instructions for follow-up, and any \"return precautions\" mentioned (e.g., \"return if symptoms worsen\").]\n",
    "\n",
    "    Consults/Referrals: [List any consultations or referrals to specialists or other providers planned by the provider.]   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:09.968686Z",
     "iopub.status.busy": "2025-04-16T21:39:09.968394Z",
     "iopub.status.idle": "2025-04-16T21:39:09.985768Z",
     "shell.execute_reply": "2025-04-16T21:39:09.984976Z",
     "shell.execute_reply.started": "2025-04-16T21:39:09.968666Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "def generate_soap_note(state: AgentState):\n",
    "    transcription = state.get(\"transcription\")\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[transcription],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1,\n",
    "            system_instruction=soap_note_generation_sys_prompt\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\"soap_note\": response.text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical intent Extraction & Structuring (from SOAP Note)\n",
    "\n",
    "clinical_intent_sys_prompt = \"\"\"\n",
    "You are a highly skilled medical information extraction specialist. Your primary task is to analyze clinical notes, specifically SOAP notes, and identify the underlying clinical intents within each section (Subjective, Objective, Assessment, Plan). You will structure these intents into a JSON format for downstream processing. You are meticulous and focused on capturing the core meaning and purpose of the clinical information, not just surface-level keywords.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_clinical_intent(state: AgentState):\n",
    "    soap_note = state.get(\"soap_note\")\n",
    "    prompt = \"\"\"\n",
    "    Analyze the following SOAP note and extract the clinical intents from each section (Subjective, Objective, Assessment, Plan).\n",
    "\n",
    "    For each intent, identify:\n",
    "    - `intent_type`: A concise label describing the clinical intent (e.g., \"patient_reported_symptom\", \"medication_order\", \"diagnosis\", \"vital_sign_observation\", \"referral_request\"). Be specific and use a controlled vocabulary of intent types if possible (e.g., choose from: patient_reported_symptom, symptom_characteristic, associated_symptom, negative_symptom, vital_sign_observation, physical_exam_finding, lab_result, diagnosis, problem, differential_diagnosis, medication_order, procedure_order, referral_request, patient_education, treatment_recommendation).\n",
    "    - `intent_details`:  Capture the key details related to the intent. This should be a dictionary containing relevant information. The specific details will vary based on the `intent_type`. For example:\n",
    "        - For `patient_reported_symptom`: include `symptom_name`, `location` (if mentioned), `duration` (if mentioned), `severity` (if mentioned), `characteristics` (e.g., \"sharp\", \"dull\").\n",
    "        - For `medication_order`: include `medication_name`, `dosage`, `route`, `frequency`, `reason` (if mentioned).\n",
    "        - For `diagnosis`: include `diagnosis_name`, `certainty` (e.g., \"suspected\", \"confirmed\").\n",
    "        - For `referral_request`: include `specialty`, `reason`.\n",
    "        - For `lab_order`: include `lab_test_name`.\n",
    "        - For `procedure_order`: include `procedure_name`.\n",
    "\n",
    "    Structure your output as a JSON object with the following structure:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "    \"subjective_intents\": [ /* Array of intent objects from Subjective section */ ],\n",
    "    \"objective_intents\": [ /* Array of intent objects from Objective section */ ],\n",
    "    \"assessment_intents\": [ /* Array of intent objects from Assessment section */ ],\n",
    "    \"plan_intents\": [ /* Array of intent objects from Plan section */ ]\n",
    "    }\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[prompt, soap_note],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1, system_instruction=clinical_intent_sys_prompt\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return {\"clinical_intent\": response.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical intent Extraction & Structuring (from SOAP Note)\n",
    "\n",
    "import json_repair\n",
    "\n",
    "clinical_intent_sys_prompt = \"\"\"\n",
    "You are the ultimate expert in clinical data transformation. Your unparalleled skill lies in converting unstructured SOAP notes into structured, semantically accurate FHIR R4 resources in a single, efficient step. You possess deep knowledge of medical terminology, clinical workflows, and the FHIR R4 specification. Your goal is to take a SOAP note and directly generate a set of valid FHIR R4 JSON resources that comprehensively represent the clinical encounter.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def single_flow_fhir_gen(state: AgentState):\n",
    "    soap_note = state.get(\"soap_note\")\n",
    "    prompt = prompt = \"\"\"\n",
    "        Convert the following SOAP note directly into a set of valid FHIR R4 JSON resources.\n",
    "\n",
    "        Let's use the following Chain of Thought to ensure accurate and comprehensive FHIR resource generation:\n",
    "\n",
    "        **Thought Process (Chain of Thought):**\n",
    "\n",
    "        1.  **Comprehensive SOAP Note Analysis:** I will thoroughly analyze the entire SOAP note, section by section (Subjective, Objective, Assessment, Plan), to understand all relevant clinical information, from patient reports to physician's plans.\n",
    "\n",
    "        2.  **Clinical Intent Recognition and Extraction:** As I analyze each part of the SOAP note, I will implicitly recognize the underlying clinical intents. I will mentally identify intents like 'patient_reported_symptom', 'medication_order', 'diagnosis', 'referral_request', etc., and extract the necessary details for each.\n",
    "\n",
    "        3.  **Direct FHIR Resource Planning and Generation:**  For each recognized clinical intent, I will directly determine the most appropriate FHIR R4 resource type *and immediately generate* the FHIR JSON content for that resource.  [**Simulated \"Action\": I will access my comprehensive FHIR R4 knowledge and best practices to select the right resource and populate it correctly.**]  **For resources representing future actions (e.g., MedicationRequest, ProcedureRequest, ServiceRequest, CarePlan, etc.), I will check if the resource has a status field that reflects operational progress. If a status field exists, I will set it to the *earliest possible status* that indicates a pending or preliminary state, such as 'draft', 'planned', 'active', 'requested', or 'on-hold', depending on the resource type and its valid status values. This ensures that these resources are not marked as complete and are ready for potential human-in-the-loop approval or further processing.** For resources that represent past or present observations or conditions, I will set their status to 'final' or appropriate terminal state.\n",
    "\n",
    "        4.  **Resource Interlinking:**  Where contextually appropriate, I will assign a globally unique identifier(UUID) to each resource and attempt to establish basic interlinks between resources. For example, linking Observations to the Encounter resources (use the placeholder reference \"unknownRef\" where the logical id is not present or unknown). I will however Assume that the Patient and Practitioner resources will be created separately and linked later.\n",
    "\n",
    "        5.  **Structured JSON Output of FHIR Resources:**  Finally, I will output a JSON object. a list of all complete generated R4 fhir resources.\n",
    "\n",
    "        **Output Format:**\n",
    "\n",
    "        Generate a JSON object where the keys are descriptive resource names and the values are the generated FHIR R4 JSON resource objects.\n",
    "\n",
    "        Example JSON output structure:\n",
    "        [<Generated Fhir resources>]\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[prompt, soap_note],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            system_instruction=clinical_intent_sys_prompt,\n",
    "            response_mime_type=\"application/json\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fhir_resources = json_repair.loads(response.text)\n",
    "    return {\"clinical_intent\": fhir_resources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fhir resource type planning(Reasoning and selection.)\n",
    "\n",
    "fhir_res_planning_sys_prompt = \"\"\"\n",
    "You are a highly experienced FHIR (Fast Healthcare Interoperability Resources) architect and clinical data modeler. Your expertise lies in understanding clinical intents and mapping them to appropriate FHIR resource types.  You are deeply familiar with the FHIR R4 specification and best practices for representing clinical information in FHIR. Your goal is to take a structured representation of clinical intents and determine the most suitable FHIR resource type for each intent to ensure accurate and semantically correct FHIR representation. You will provide a list of FHIR resource types associated with each intent.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_fhir_resource_plan(state: AgentState):\n",
    "    clinical_intent = state.get(\"clinical_intent\")\n",
    "    prompt = f\"\"\"\n",
    "Based on the following structured representation of clinical intents extracted from a SOAP note, determine the most appropriate FHIR resource type for each intent.\n",
    "\n",
    "For each clinical intent provided in the input JSON, you need to:\n",
    "- `fhir_resource_type`: Identify the single most appropriate FHIR R4 resource type to represent this clinical intent. Choose from standard FHIR resource types like: Encounter, Patient, Practitioner, Observation, Condition, MedicationRequest, Procedure, ServiceRequest, DiagnosticReport, etc. Just provide the resource type name (e.g., \"Observation\", \"ServiceRequest\").\n",
    "- `intent_source`:  Indicate the source of the intent from the input JSON (e.g., \"subjective_intents[0]\", \"plan_intents[2]\", \"overall_encounter\").  Use \"overall_encounter\" for intents that apply to the entire encounter (like creating an Encounter resource itself). Use \"patient_context\" for intents related to patient demographics. Use \"practitioner_context\" for intents related to the practitioner.\n",
    "- `intent_details`:  Carry over the `intent_details` dictionary from the input JSON for context.\n",
    "\n",
    "Consider these general mappings as guidelines:\n",
    "- `patient_reported_symptom`, `symptom_characteristic`, `associated_symptom`, `negative_symptom`, `vital_sign_observation`, `physical_exam_finding`, `lab_result` intents usually map to `Observation`.\n",
    "- `diagnosis`, `problem`, `differential_diagnosis` intents usually map to `Condition`.\n",
    "- `medication_order` intents usually map to `MedicationRequest`.\n",
    "- `procedure_order` intents usually map to `ServiceRequest` (or sometimes `Procedure` depending on context).\n",
    "- `referral_request` intents usually map to `ServiceRequest` with category 'referral'.\n",
    "- Overall encounter information should map to `Encounter`.\n",
    "- Patient demographics and identifiers should map to `Patient`.\n",
    "- Practitioner information should map to `Practitioner`.\n",
    "\n",
    "Input JSON (Structured Clinical Intents):\n",
    "```json\n",
    "{clinical_intent}\n",
    "```\n",
    "\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[prompt],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1, system_instruction=fhir_res_planning_sys_prompt\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return {\"resource_plan\": response.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the fhir resources\n",
    "gen_fhir_res_sys_prompt = \"\"\"\n",
    "You are a meticulous FHIR JSON generator. You are an expert at creating valid and well-formed FHIR R4 JSON resources. Your task is to take a specification of FHIR resource types and associated clinical intent details and generate the full JSON content for each resource. You pay close attention to FHIR R4 structure, data types, and best practices. You aim for complete and semantically accurate FHIR resources based on the provided intent information.  Where appropriate, you will try to include relevant coding (e.g., SNOMED CT, LOINC) if you have access to or can infer suitable codes, but prioritize generating valid FHIR structure even if coding is not fully resolved.\n",
    "    \"\"\"\n",
    "\n",
    "def generate_fhir_resources(state: AgentState):\n",
    "    resource_plan = state.get(\"resource_plan\")\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate the FHIR R4 JSON content for each FHIR resource type specified in the input JSON array.  For each resource type, use the associated `intent_details` to populate the relevant fields in the FHIR resource.\n",
    "\n",
    "    Input JSON Array (FHIR Resource Types and Intents):\n",
    "    ```json\n",
    "    {resource_plan}\n",
    "    ```\n",
    "\n",
    "    For each object in the input array, generate a corresponding FHIR R4 JSON resource. Ensure:\n",
    "\n",
    "    The resourceType field is correctly set.\n",
    "\n",
    "    Use appropriate FHIR data types for each element.\n",
    "\n",
    "    For resources like Observation and ServiceRequest, try to include relevant coding where possible (e.g., for Observation.code, ServiceRequest.code, ServiceRequest.category). If you can infer relevant SNOMED CT or LOINC codes based on the intent_details, include them in coding elements. If not, at least include text elements with descriptive names.\n",
    "\n",
    "    For resources that need references (e.g., Observation.subject, ServiceRequest.subject, Encounter.subject, Encounter.participant.individual), use placeholder references like:{{\"reference\": \"Patient/example\"}}, {{\"reference\": \"Practitioner/example\"}}. Assume Patient and Practitioner resources will be created separately and linked later.\n",
    "\n",
    "    For Encounter resources, assume a status of \"planned\", class of \"ambulatory\", and use the first listed chief complaint from the Subjective intents as the reasonCode.text.\n",
    "\n",
    "    Output a JSON object where the keys are descriptive resource names (e.g., \"Observation_ChestPain\", \"ServiceRequest_CBC\", \"Encounter\") and the values are the generated FHIR JSON resource objects.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[user_prompt],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1,\n",
    "            system_instruction=gen_fhir_res_sys_prompt\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\"fhir_resources\": response.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:09.989299Z",
     "iopub.status.busy": "2025-04-16T21:39:09.989051Z",
     "iopub.status.idle": "2025-04-16T21:39:10.000739Z",
     "shell.execute_reply": "2025-04-16T21:39:09.999648Z",
     "shell.execute_reply.started": "2025-04-16T21:39:09.989278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def evaluate_outputs(state: AgentState):\n",
    "#     # patient_emr, audio_file, questionnaire_response, soap_note, questionnaire_example, soap_example\n",
    "#     patient_emr = None # state.get(\"patient_emr\")\n",
    "#     audio_file = state.get(\"uploaded_audio_file\")\n",
    "#     questionnaire_response = state.get(\"quest_resp\")\n",
    "#     soap_note = state.get(\"soap_note\")\n",
    "#     questionnaire_example = state.get(\"quest\")\n",
    "#     soap_note_sample = state.get(\"soap_note_sample\")\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a medical evaluator tasked with reviewing clinical documentation.\n",
    "\n",
    "#     You will evaluate:\n",
    "#     1. The quality and completeness of a **Questionnaire Response**\n",
    "#     2. The accuracy and structure of a **SOAP Note**\n",
    "\n",
    "#     @Lakshay - TODO - what does quality, accuracy and structure look like???\n",
    "#     Use the following scale:\n",
    "#     - 5 = Very Good\n",
    "#     - 4 = Good\n",
    "#     - 3 = Acceptable\n",
    "#     - 2 = Poor\n",
    "#     - 1 = Very Poor\n",
    "#     - 0 = Unusable\n",
    "\n",
    "#     Base your evaluation on:\n",
    "#     - Clinical relevance and coherence\n",
    "#     - Completeness compared to examples\n",
    "#     - Consistency with the patient EMR\n",
    "\n",
    "#     --- Patient EMR (for reference) ---\n",
    "#     {patient_emr}\n",
    "\n",
    "#     --- Audio Transcript of patient and Doctor Conversation (for reference) ---\n",
    "#     {audio_file}\n",
    "\n",
    "#     --- Example Questionnaire Response ---\n",
    "#     {questionnaire_example}\n",
    "\n",
    "#     --- Given Questionnaire Response ---\n",
    "#     {questionnaire_response}\n",
    "\n",
    "#     --- Example SOAP Note ---\n",
    "#     {soap_note_sample}\n",
    "\n",
    "#     --- Given SOAP Note ---\n",
    "#     {soap_note}\n",
    "\n",
    "#     Now provide a rating for each of the following:\n",
    "#     1. Questionnaire Response (0–5):\n",
    "#     2. SOAP Note (0–5):\n",
    "\n",
    "#     Include a one-sentence rationale for each score.\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.models.generate_content(\n",
    "#         model=model_id,\n",
    "#         contents=[prompt],\n",
    "#         config=types.GenerateContentConfig(\n",
    "#             temperature=0,\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:10.001737Z",
     "iopub.status.busy": "2025-04-16T21:39:10.001503Z",
     "iopub.status.idle": "2025-04-16T21:39:10.014603Z",
     "shell.execute_reply": "2025-04-16T21:39:10.013819Z",
     "shell.execute_reply.started": "2025-04-16T21:39:10.001720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def truncate_text(text: str, n: int = 50) -> str:\n",
    "    \"\"\"Truncates text to the first n characters, adding ellipsis if truncated.\"\"\"\n",
    "    if len(text) > n:\n",
    "        return text[:n] + \"...\"\n",
    "    return text\n",
    "\n",
    "def write_response(state: AgentState):\n",
    "    \"\"\"\n",
    "    Writes a response to the user based on the agent's state, reporting the outcome\n",
    "    of the workflow.\n",
    "\n",
    "    This function serves as the reporting node in the LangGraph workflow. It examines\n",
    "    the AgentState to determine the success or failure\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent, containing information\n",
    "                            about the workflow execution, including questionnaire\n",
    "                            response and SOAP note.\n",
    "    \"\"\"\n",
    "\n",
    "    print(truncate_text(state.get(\"transcription\")))\n",
    "    print(state.get(\"soap_note\"))\n",
    "    print(state.get(\"clinical_intent\"))\n",
    "    print(state.get(\"resource_plan\"))\n",
    "    print(state.get(\"fhir_resources\"))\n",
    "    quest_resp = state.get(\"quest_resp\")\n",
    "    soap_note = state.get(\"soap_note\")\n",
    "    quest_found = state.get(\"quest_found\")\n",
    "\n",
    "    if quest_resp is None or soap_note is None:\n",
    "        error_message = \"Workflow encountered an issue.\"\n",
    "        if not quest_found:\n",
    "            error_message += \" It appears there was a problem finding relevant questionnaire information. \"\n",
    "        if quest_resp is None:\n",
    "            error_message += \"Questionnaire Response was not generated. \"\n",
    "        if soap_note is None:\n",
    "            error_message += \"SOAP Note was not generated.\"\n",
    "        print(error_message)\n",
    "    else:\n",
    "        truncated_quest_resp = truncate_text(json.dumps(quest_resp, indent=2))\n",
    "        truncated_soap_note = truncate_text(soap_note)\n",
    "\n",
    "        success_message = \"Workflow completed successfully!\\n\\n\"\n",
    "        success_message += \"**Questionnaire Response:**\\n\"\n",
    "        success_message += truncated_quest_resp + \"\\n\\n\"\n",
    "        success_message += \"**SOAP Note:**\\n\"\n",
    "        success_message += truncated_soap_note\n",
    "        print(success_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring everything together as a graph to execture our workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:10.015800Z",
     "iopub.status.busy": "2025-04-16T21:39:10.015549Z",
     "iopub.status.idle": "2025-04-16T21:39:10.051369Z",
     "shell.execute_reply": "2025-04-16T21:39:10.050656Z",
     "shell.execute_reply.started": "2025-04-16T21:39:10.015779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI # Correct import path\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from typing import Literal\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=model_id, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Defined the graph\n",
    "wk_graph = StateGraph(AgentState)\n",
    "\n",
    "def aggregate_state(state: StateGraph):\n",
    "    return state\n",
    "\n",
    "# node magic strings\n",
    "audio_file_upload_key = \"upload_to_gemini\"\n",
    "gen_soap_note_key = \"generate_soap_note\"\n",
    "write_resp_key = \"write_resp\"\n",
    "state_aggregator_key = \"state_aggregator\"\n",
    "diarize_audio_key = \"diarize_audio\"\n",
    "clinical_intent_key=\"get_clinical_intent\"\n",
    "fhir_res_planning_key=\"get_fhir_resource_plan\"\n",
    "gen_fhir_resources_key=\"generate_fhir_resources\"\n",
    "\n",
    "single_flow_fhir_gen_key=\"single_flow_fhir_gen\"\n",
    "\n",
    "\n",
    "# Nodes\n",
    "wk_graph.add_node(audio_file_upload_key, upload_to_gemini)\n",
    "wk_graph.add_node(gen_soap_note_key, generate_soap_note)\n",
    "wk_graph.add_node(write_resp_key, write_response)\n",
    "wk_graph.add_node(state_aggregator_key, aggregate_state)\n",
    "wk_graph.add_node(diarize_audio_key, diarize_audio)\n",
    "wk_graph.add_node(clinical_intent_key, get_clinical_intent)\n",
    "wk_graph.add_node(fhir_res_planning_key, get_fhir_resource_plan)\n",
    "wk_graph.add_node(gen_fhir_resources_key, generate_fhir_resources)\n",
    "\n",
    "wk_graph.add_node(single_flow_fhir_gen_key, single_flow_fhir_gen)\n",
    "\n",
    "def check_file_upload(state: AgentState):\n",
    "    if state.get(\"uploaded_audio_file\"):\n",
    "        return diarize_audio_key\n",
    "    else:\n",
    "        return write_resp_key\n",
    "\n",
    "\n",
    "# Edges\n",
    "wk_graph.add_edge(START, audio_file_upload_key)\n",
    "wk_graph.add_conditional_edges(audio_file_upload_key, check_file_upload)\n",
    "wk_graph.add_edge(diarize_audio_key, gen_soap_note_key)\n",
    "wk_graph.add_edge(gen_soap_note_key, single_flow_fhir_gen_key)\n",
    "# wk_graph.add_edge(gen_soap_note_key, clinical_intent_key)\n",
    "# wk_graph.add_edge(clinical_intent_key, fhir_res_planning_key)\n",
    "# wk_graph.add_edge(fhir_res_planning_key, gen_fhir_resources_key)\n",
    "# wk_graph.add_edge(gen_fhir_resources_key, write_resp_key)\n",
    "wk_graph.add_edge(single_flow_fhir_gen_key, write_resp_key)\n",
    "wk_graph.add_edge(write_resp_key, END)\n",
    "\n",
    "graph = wk_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:10.052457Z",
     "iopub.status.busy": "2025-04-16T21:39:10.052213Z",
     "iopub.status.idle": "2025-04-16T21:39:10.102580Z",
     "shell.execute_reply": "2025-04-16T21:39:10.101752Z",
     "shell.execute_reply.started": "2025-04-16T21:39:10.052437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# Image(graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T21:39:10.103657Z",
     "iopub.status.busy": "2025-04-16T21:39:10.103416Z",
     "iopub.status.idle": "2025-04-16T21:39:11.463290Z",
     "shell.execute_reply": "2025-04-16T21:39:11.462171Z",
     "shell.execute_reply.started": "2025-04-16T21:39:10.103640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio_file_path = '/kaggle/input/quest-sample-db/RES0005.mp3'\n",
    "audio_file_path = \"/home/peter/Desktop/gdaCapstone/Data/Audio Recordings/CAR0002.mp3\"\n",
    "\n",
    "inputs = {\n",
    "    \"audio_file_path\": audio_file_path,\n",
    "}\n",
    "graph.invoke(inputs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7114367,
     "sourceId": 11400356,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7164736,
     "sourceId": 11438045,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
